{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b858b9-8003-462d-80f5-cbf369c9f61e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:38:16.425420Z",
     "iopub.status.busy": "2024-07-05T10:38:16.424440Z",
     "iopub.status.idle": "2024-07-05T10:38:18.656247Z",
     "shell.execute_reply": "2024-07-05T10:38:18.654749Z",
     "shell.execute_reply.started": "2024-07-05T10:38:16.425344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "参考：  \n",
    "1. https://github.com/yuanzhoulvpi2017/zero_nlp/wiki/%E4%BB%8Esft_clm_mlm%E4%B8%89%E7%A7%8D%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F%E6%9D%A5%E7%9C%8Bdata_collator%E2%80%94%E2%80%94%E3%80%90transformers%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%91\n",
    "2. https://huggingface.co/learn/nlp-course/zh-CN/chapter7/6\n",
    "\n",
    "\n",
    "利用指令微调的数据（包括：instruction、input、output等字段的数据样本）进行持续预训练，进行的是因果语言模型（CausalLM）， \n",
    "需要注意的是：在一般的因果语言模型CLM中，input_ids和label_ids偏移一个位置， 然后组成数据进行预训练\n",
    "在基于指令微调数据的语言模型sft中，input_ids是instruction和input组成的source, output组成的target\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "cache_dir = \"/root/autodl-tmp/\"\n",
    "import transformers \n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42601fc0-a766-4311-961a-529d42e43f82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:38:41.953388Z",
     "iopub.status.busy": "2024-07-05T10:38:41.952672Z",
     "iopub.status.idle": "2024-07-05T10:38:44.306843Z",
     "shell.execute_reply": "2024-07-05T10:38:44.305310Z",
     "shell.execute_reply.started": "2024-07-05T10:38:41.953350Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging, torch, click\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from functools import partial\n",
    "\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, PreTrainedTokenizer, Trainer, TrainingArguments, set_seed, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "from consts import DEFAULT_INPUT_MODEL, DEFAULT_SEED, PROMPT_WITH_INPUT_FORMAT, PROMPT_NO_INPUT_FORMAT, END_KEY, INSTRUCTION_KEY, RESPONSE_KEY, INTRO_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeda3eb1-90a1-4b08-80cf-2cac8197d109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:38:44.309380Z",
     "iopub.status.busy": "2024-07-05T10:38:44.308862Z",
     "iopub.status.idle": "2024-07-05T10:38:44.315379Z",
     "shell.execute_reply": "2024-07-05T10:38:44.314197Z",
     "shell.execute_reply.started": "2024-07-05T10:38:44.309342Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ROOT_PATH = \"llm-tutorial/llm-ie/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082bdf8b-122c-487b-ae9b-2cbe28fce2f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:38:44.316630Z",
     "iopub.status.busy": "2024-07-05T10:38:44.316388Z",
     "iopub.status.idle": "2024-07-05T10:38:44.325327Z",
     "shell.execute_reply": "2024-07-05T10:38:44.324271Z",
     "shell.execute_reply.started": "2024-07-05T10:38:44.316602Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        # The prompt ends with the response key plus a newline.  We encode this and then try to find it in the\n",
    "        # sequence of tokens.  This should just be a single token.\n",
    "        response_token_ids = self.tokenizer(RESPONSE_KEY)[\"input_ids\"]\n",
    "\n",
    "        labels = batch[\"labels\"].clone()\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "\n",
    "            response_token_ids_start_idx = None\n",
    "            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n",
    "                response_token_ids_start_idx = idx\n",
    "                break\n",
    "\n",
    "            if response_token_ids_start_idx is None:\n",
    "                raise RuntimeError(\n",
    "                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n",
    "                )\n",
    "\n",
    "            response_token_ids_end_idx = response_token_ids_start_idx + 1\n",
    "\n",
    "            # Make pytorch loss function ignore all tokens up through the end of the response key\n",
    "            labels[i, :response_token_ids_end_idx] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ab03b6-53f7-44ed-8035-a4b513e60fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:38:44.954395Z",
     "iopub.status.busy": "2024-07-05T10:38:44.953673Z",
     "iopub.status.idle": "2024-07-05T10:38:50.582415Z",
     "shell.execute_reply": "2024-07-05T10:38:50.580642Z",
     "shell.execute_reply.started": "2024-07-05T10:38:44.954351Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02887c9adb814ac5802256e348563822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_batch(batch: Dict[str, List], tokenizer: AutoTokenizer, max_length: int) -> dict:\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_training_dataset(path_or_dataset: str = \"data/yayi_train_example.json\") -> Dataset:\n",
    "    logger.info(f\"Loading dataset from {path_or_dataset}\")\n",
    "    dataset = load_dataset(\"json\", data_files=path_or_dataset)[\"train\"]\n",
    "    print(dataset)\n",
    "    logger.info(\"Found %d rows\", dataset.num_rows)\n",
    "\n",
    "    def _add_text(rec):\n",
    "        instruction = rec[\"instruction\"]\n",
    "        context = rec.get(\"input\", \"\")\n",
    "        response = rec[\"output\"]\n",
    "\n",
    "        if not instruction:\n",
    "            raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "        if not response:\n",
    "            raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "        # For some instructions there is an input that goes along with the instruction, providing context for the\n",
    "        # instruction.  For example, the input might be a passage from Wikipedia and the instruction says to extract\n",
    "        # some piece of information from it.  The response is that information to extract.  In other cases there is\n",
    "        # no input.  For example, the instruction might be open QA such as asking what year some historic figure was\n",
    "        # born.\n",
    "        if context:\n",
    "            rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n",
    "        else:\n",
    "            rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n",
    "        return rec\n",
    "\n",
    "    dataset = dataset.map(_add_text)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_tokenizer(pretrained_model_name_or_path: str = DEFAULT_INPUT_MODEL) -> PreTrainedTokenizer:\n",
    "    logger.info(f\"Loading tokenizer for {pretrained_model_name_or_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, cache_dir=os.path.join(cache_dir, \"yayi_7b_model\"))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [INTRO_KEY, INSTRUCTION_KEY, RESPONSE_KEY, END_KEY]})\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    pretrained_model_name_or_path: str = DEFAULT_INPUT_MODEL, *, gradient_checkpointing: bool = False\n",
    ") -> AutoModelForCausalLM:\n",
    "    logger.info(f\"Loading model for {pretrained_model_name_or_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path, trust_remote_code=True, torch_dtype=torch.float16, cache_dir=os.path.join(cache_dir, \"yayi_7b_model\"), use_cache=False if gradient_checkpointing else True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_tokenizer(\n",
    "    pretrained_model_name_or_path: str = DEFAULT_INPUT_MODEL, *, gradient_checkpointing: bool = False\n",
    ") -> Tuple[AutoModelForCausalLM, PreTrainedTokenizer]:\n",
    "    tokenizer = load_tokenizer(pretrained_model_name_or_path)\n",
    "    model = load_model(pretrained_model_name_or_path, gradient_checkpointing=gradient_checkpointing)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed=DEFAULT_SEED, path_or_dataset=None) -> Dataset:\n",
    "    \"\"\"Loads the training dataset and tokenizes it so it is ready for training.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (AutoTokenizer): Tokenizer tied to the model.\n",
    "        max_length (int): Maximum number of tokens to emit from tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = load_training_dataset(path_or_dataset=path_or_dataset)\n",
    "\n",
    "    logger.info(\"Preprocessing dataset\")\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"input\", \"output\", \"text\"],\n",
    "    )\n",
    "    logger.info(f\"datasets after processing: {dataset}\")\n",
    "\n",
    "    # Make sure we don't have any truncated records, as this would mean the end keyword is missing.\n",
    "    logger.info(\"Processed dataset has %d rows\", dataset.num_rows)\n",
    "    # dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
    "    logger.info(\"Processed dataset has %d rows after filtering for truncated records\", dataset.num_rows)\n",
    "\n",
    "    logger.info(\"Shuffling dataset\")\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    logger.info(\"Done preprocessing\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "set_seed(DEFAULT_SEED)\n",
    "model, tokenizer = get_model_tokenizer(DEFAULT_INPUT_MODEL, gradient_checkpointing=False)\n",
    "\n",
    "\n",
    "for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "    max_length = getattr(model.config, length_setting, None)\n",
    "    if max_length:\n",
    "        logger.info(f\"Found max lenth: {max_length}\")\n",
    "        break\n",
    "if not max_length:\n",
    "    max_length = 1024\n",
    "    logger.info(f\"Using default max length: {max_length}\")\n",
    "\n",
    "\n",
    "max_length = 1024 * 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a357d3-1968-467a-a955-73262df4698f",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T10:38:50.584649Z",
     "iopub.status.busy": "2024-07-05T10:38:50.584330Z",
     "iopub.status.idle": "2024-07-05T10:38:51.491330Z",
     "shell.execute_reply": "2024-07-05T10:38:51.489910Z",
     "shell.execute_reply.started": "2024-07-05T10:38:50.584617Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': '',\n",
       " 'instruction': '你是谁',\n",
       " 'input': '',\n",
       " 'output': '我的中文名是雅意，英文名是YaYi，是一个由中科闻歌算法团队训练的语言模型'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import default_data_collator  # 默认是因果语言模型\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"yayi_train_example.json\")[\"train\"]\n",
    "dataset[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6b60e0-fc5a-4650-9eee-cc668a516db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:38:51.492886Z",
     "iopub.status.busy": "2024-07-05T10:38:51.492522Z",
     "iopub.status.idle": "2024-07-05T10:38:52.659337Z",
     "shell.execute_reply": "2024-07-05T10:38:52.658183Z",
     "shell.execute_reply.started": "2024-07-05T10:38:51.492857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['system', 'instruction', 'input', 'output'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processed_dataset = preprocess_dataset(tokenizer=tokenizer, max_length=max_length, seed=DEFAULT_SEED, path_or_dataset=\"yayi_train_example.json\")\n",
    "\n",
    "\n",
    "split_dataset = processed_dataset.train_test_split(test_size=0.1, seed=DEFAULT_SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f9145f-d48e-448d-b20e-f0d0d3e19c36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:39:07.499807Z",
     "iopub.status.busy": "2024-07-05T10:39:07.499245Z",
     "iopub.status.idle": "2024-07-05T10:39:07.506446Z",
     "shell.execute_reply": "2024-07-05T10:39:07.505315Z",
     "shell.execute_reply.started": "2024-07-05T10:39:07.499774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|System|>:\n",
      "A chat between a human and an AI assistant named YaYi.\n",
      "YaYi is a helpful and harmless language model developed by Beijing Wenge Technology Co.,Ltd.\n",
      "\n",
      "<|Human|>:\n",
      "你是谁\n",
      "\n",
      "<|YaYi|>:\n",
      "我是大模型\n",
      "\n",
      "<|End|>\n"
     ]
    }
   ],
   "source": [
    "print(PROMPT_NO_INPUT_FORMAT.format(instruction=\"你是谁\", response=\"我是大模型\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a53e1c-7246-4793-a793-845450959834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:39:07.576135Z",
     "iopub.status.busy": "2024-07-05T10:39:07.575191Z",
     "iopub.status.idle": "2024-07-05T10:39:07.586339Z",
     "shell.execute_reply": "2024-07-05T10:39:07.584539Z",
     "shell.execute_reply.started": "2024-07-05T10:39:07.576061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|System|>:\n",
      "A chat between a human and an AI assistant named YaYi.\n",
      "YaYi is a helpful and harmless language model developed by Beijing Wenge Technology Co.,Ltd.\n",
      "\n",
      "<|Human|>:\n",
      "你是谁\n",
      "训练中\n",
      "\n",
      "<|YaYi|>:\n",
      "我是大模型\n",
      "\n",
      "<|End|>\n"
     ]
    }
   ],
   "source": [
    "print(PROMPT_WITH_INPUT_FORMAT.format(instruction=\"你是谁\", response=\"我是大模型\", input=\"训练中\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "284df424-67ce-4eae-a69c-85ffd4040ccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:43:45.649530Z",
     "iopub.status.busy": "2024-07-05T10:43:45.648707Z",
     "iopub.status.idle": "2024-07-05T10:43:45.682354Z",
     "shell.execute_reply": "2024-07-05T10:43:45.679937Z",
     "shell.execute_reply.started": "2024-07-05T10:43:45.649452Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[0:2]\n",
    "\n",
    "from typing import Mapping\n",
    "\n",
    "\n",
    "isinstance(split_dataset[\"train\"][0], (Mapping,))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9935c4ac-d72b-4a6f-8594-0e050c635683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:43:58.782575Z",
     "iopub.status.busy": "2024-07-05T10:43:58.780634Z",
     "iopub.status.idle": "2024-07-05T10:43:58.791850Z",
     "shell.execute_reply": "2024-07-05T10:43:58.790185Z",
     "shell.execute_reply.started": "2024-07-05T10:43:58.782491Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 训练过程\n",
    "\n",
    "\n",
    "set_seed(DEFAULT_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbcb3c9e-2929-4dcc-8593-43b4d02f2c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T10:08:07.603190Z",
     "iopub.status.busy": "2024-07-05T10:08:07.602617Z",
     "iopub.status.idle": "2024-07-05T10:08:07.621318Z",
     "shell.execute_reply": "2024-07-05T10:08:07.619480Z",
     "shell.execute_reply.started": "2024-07-05T10:08:07.603157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mOnlyLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mresponse_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minstruction_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmlm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Initialize self.  See help(type(self)) for accurate signature.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mresponse_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minstruction_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmlm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstruction_template\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# The user provides a string, must tokenize\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# The user already provides the token ids\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstruction_template\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_template\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# The user provides a string, must tokenize\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# The user already provides the token ids\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_template\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_template\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"The pad_token_id and eos_token_id values of this tokenizer are identical. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"If you are planning for multi-turn training, \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"it can result in the model continuously generating questions and answers without eos token. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"To avoid this, set the pad_token_id to a different value.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.10/site-packages/trl/trainer/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from trl import DataCollatorForCompletionOnlyLM as OnlyLM  # 主要作用于续写\n",
    "\"\"\"\n",
    "是一个用于自然语言处理任务中的特定数据批处理工具，特别是在语言模型的完成（completion）任务中。在这种任务中，模型的目标是生成给定前缀或提示的文本的续写部分。\n",
    "\"\"\"\n",
    "\n",
    "texts = [\"我是谁\", \"我喜欢上海\" \"我\"]\n",
    "\n",
    "OnlyLM.__init__??\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c11ffed1-d675-44ea-87be-85e09ab6e8b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T11:02:59.466045Z",
     "iopub.status.busy": "2024-07-05T11:02:59.465659Z",
     "iopub.status.idle": "2024-07-05T11:02:59.472316Z",
     "shell.execute_reply": "2024-07-05T11:02:59.471293Z",
     "shell.execute_reply.started": "2024-07-05T11:02:59.466014Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "分析多轮度化的模版\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06dc4a25-b8ba-4def-a7ab-c8505ff9bf42",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-07-05T11:08:31.242581Z",
     "iopub.status.busy": "2024-07-05T11:08:31.241428Z",
     "iopub.status.idle": "2024-07-05T11:08:32.097261Z",
     "shell.execute_reply": "2024-07-05T11:08:32.095955Z",
     "shell.execute_reply.started": "2024-07-05T11:08:31.242547Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [104198, 100165], 'attention_mask': [1, 1]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tk = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\", cache_dir=\"/root/autodl-tmp/qwen2-7b-instruct/\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"\"}, {}, {}, {}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
