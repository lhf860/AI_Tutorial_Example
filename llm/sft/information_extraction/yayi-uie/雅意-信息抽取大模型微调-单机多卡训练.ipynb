{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a960743e-b2be-4119-a422-f641feafeaf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:12:57.449724Z",
     "iopub.status.busy": "2024-07-11T05:12:57.448410Z",
     "iopub.status.idle": "2024-07-11T05:13:01.569267Z",
     "shell.execute_reply": "2024-07-11T05:13:01.568062Z",
     "shell.execute_reply.started": "2024-07-11T05:12:57.449672Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "import torch, sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "from datasets import load_dataset, Dataset\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c8b9bd-dd45-461a-88e5-e79f424345dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:13:01.571446Z",
     "iopub.status.busy": "2024-07-11T05:13:01.571059Z",
     "iopub.status.idle": "2024-07-11T05:13:01.650410Z",
     "shell.execute_reply": "2024-07-11T05:13:01.649311Z",
     "shell.execute_reply.started": "2024-07-11T05:13:01.571415Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at , will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mwrite_basic_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmixed_precision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'no'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_location\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/root/.cache/huggingface/accelerate/default_config.yaml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_xpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mwrite_basic_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"no\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_location\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_json_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_xpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Creates and saves a basic cluster config to be used on a local machine with potentially multiple GPUs. Will also\u001b[0m\n",
       "\u001b[0;34m    set CPU if it is a CPU-only machine.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        mixed_precision (`str`, *optional*, defaults to \"no\"):\u001b[0m\n",
       "\u001b[0;34m            Mixed Precision to use. Should be one of \"no\", \"fp16\", or \"bf16\"\u001b[0m\n",
       "\u001b[0;34m        save_location (`str`, *optional*, defaults to `default_json_config_file`):\u001b[0m\n",
       "\u001b[0;34m            Optional custom save location. Should be passed to `--config_file` when using `accelerate launch`. Default\u001b[0m\n",
       "\u001b[0;34m            location is inside the huggingface cache folder (`~/.cache/huggingface`) but can be overriden by setting\u001b[0m\n",
       "\u001b[0;34m            the `HF_HOME` environmental variable, followed by `accelerate/default_config.yaml`.\u001b[0m\n",
       "\u001b[0;34m        use_xpu (`bool`, *optional*, defaults to `False`):\u001b[0m\n",
       "\u001b[0;34m            Whether to use XPU if available.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Configuration already exists at {save_location}, will not override. Run `accelerate config` manually or pass a different `save_location`.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmixed_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixed_precision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mmixed_precision\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"no\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fp16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bf16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fp8\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"`mixed_precision` should be one of 'no', 'fp16', 'bf16', or 'fp8'. Received {mixed_precision}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"compute_environment\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"LOCAL_MACHINE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"mixed_precision\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmixed_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mis_mlu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnum_mlus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_processes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_mlus\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cpu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnum_mlus\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distributed_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MULTI_MLU\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distributed_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NO\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnum_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_processes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_gpus\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cpu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnum_gpus\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distributed_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MULTI_GPU\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distributed_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NO\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melif\u001b[0m \u001b[0mis_xpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_xpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnum_xpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_processes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_xpus\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cpu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnum_xpus\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distributed_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MULTI_XPU\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distributed_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NO\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melif\u001b[0m \u001b[0mis_npu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnum_npus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_processes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_npus\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cpu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnum_npus\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distributed_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MULTI_NPU\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distributed_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NO\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnum_xpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cpu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_processes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distributed_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NO\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"debug\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enable_cpu_affinity\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClusterConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.10/site-packages/accelerate/commands/config/default.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config??\n",
    "\n",
    "write_basic_config(mixed_precision=\"\",  # 是否使用混合精度， 默认是no，可以设置：no， fp16, bf16\n",
    "                  save_location=\"\",  # accelerate的配置文件保存地址，默认：/root/.cache/huggingface/accelerate/default_config.yaml\n",
    "                  use_xpu=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35fe8ad-d6f9-4637-89fa-8a113d5925dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:13:01.660600Z",
     "iopub.status.busy": "2024-07-11T05:13:01.660391Z",
     "iopub.status.idle": "2024-07-11T05:13:02.659502Z",
     "shell.execute_reply": "2024-07-11T05:13:02.658703Z",
     "shell.execute_reply.started": "2024-07-11T05:13:01.660574Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dataset': 'DuEE1.0',\n",
      " 'Task': 'CEE',\n",
      " 'input': '',\n",
      " 'instruction': '文本: 男子以帮找工作名义骗钱被警方抓获 \\n'\n",
      "                \"【事件抽取】事件类型列表=['财经/交易-出售/收购', '财经/交易-跌停', '财经/交易-加息', \"\n",
      "                \"'财经/交易-降价', '财经/交易-降息', '财经/交易-融资', '财经/交易-上市', '财经/交易-涨价', \"\n",
      "                \"'财经/交易-涨停', '产品行为-发布', '产品行为-获奖', '产品行为-上映', '产品行为-下架', \"\n",
      "                \"'产品行为-召回', '交往-道歉', '交往-点赞', '交往-感谢', '交往-会见', '交往-探班', \"\n",
      "                \"'竞赛行为-夺冠', '竞赛行为-晋级', '竞赛行为-禁赛', '竞赛行为-胜负', '竞赛行为-退赛', \"\n",
      "                \"'竞赛行为-退役', '人生-产子/女', '人生-出轨', '人生-订婚', '人生-分手', '人生-怀孕', \"\n",
      "                \"'人生-婚礼', '人生-结婚', '人生-离婚', '人生-庆生', '人生-求婚', '人生-失联', \"\n",
      "                \"'人生-死亡', '司法行为-罚款', '司法行为-拘捕', '司法行为-举报', '司法行为-开庭', \"\n",
      "                \"'司法行为-立案', '司法行为-起诉', '司法行为-入狱', '司法行为-约谈', '灾害/意外-爆炸', \"\n",
      "                \"'灾害/意外-车祸', '灾害/意外-地震', '灾害/意外-洪灾', '灾害/意外-起火', '灾害/意外-坍/垮塌', \"\n",
      "                \"'灾害/意外-袭击', '灾害/意外-坠机', '组织关系-裁员', '组织关系-辞/离职', '组织关系-加盟', \"\n",
      "                \"'组织关系-解雇', '组织关系-解散', '组织关系-解约', '组织关系-停职', '组织关系-退出', \"\n",
      "                \"'组织行为-罢工', '组织行为-闭幕', '组织行为-开幕', '组织行为-游行']，论元角色列表=['刑期', \"\n",
      "                \"'领投方', '立案机构', '怀孕者', '失联者', '融资金额', '出生者', '产子者', '地点', \"\n",
      "                \"'罢工人员', '原所属组织', '融资轮次', '会见主体', '致谢人', '庆祝方', '加息幅度', '加盟者', \"\n",
      "                \"'开庭案件', '受伤人数', '加息机构', '约谈对象', '奖项', '发布方', '胜者', '裁员人数', \"\n",
      "                \"'禁赛机构', '上映方', '坍塌主体', '死者', '死亡人数', '退役者', '被下架方', '订婚主体', \"\n",
      "                \"'震级', '开庭法院', '赛事名称', '降价幅度', '举报对象', '交易物', '会见对象', '生日方年龄', \"\n",
      "                \"'拘捕者', '解约方', '降价物', '被拘捕者', '被感谢人', '被解约方', '裁员方', '道歉对象', \"\n",
      "                \"'被告', '求婚者', '降价方', '罢工人数', '夺冠赛事', '执法机构', '探班主体', '罚款对象', \"\n",
      "                \"'探班对象', '死者年龄', '袭击对象', '收购方', '被解雇人员', '获奖人', '解散方', '跌停股票', \"\n",
      "                \"'解雇方', '退赛赛事', '震源深度', '入狱者', '约谈发起方', '召回内容', '颁奖机构', '退赛方', \"\n",
      "                \"'生日方', '退出方', '出售价格', '禁赛时长', '所加盟组织', '立案对象', '游行人数', '融资方', \"\n",
      "                \"'活动名称', '出售方', '降息幅度', '上映影视', '袭击者', '游行组织', '涨停股票', '降息机构', \"\n",
      "                \"'时间', '出轨方', '出轨对象', '离职者', '道歉者', '所属组织', '上市企业', '震中', \"\n",
      "                \"'发布产品', '原告', '结婚双方', '涨价方', '点赞对象', '参礼人员', '下架产品', '涨价幅度', \"\n",
      "                \"'被禁赛人员', '离婚双方', '涨价物', '跟投方', '点赞方', '晋级方', '晋级赛事', '求婚对象', \"\n",
      "                \"'败者', '下架方', '召回方', '停职人员', '分手双方', '罚款金额', '举报发起方', \"\n",
      "                \"'冠军']。我希望你根据事件类型列表和论元角色列表从给定的文本中抽取可能的事件，并以json[{'trigger':'', \"\n",
      "                \"'type':'', 'arguments': {角色:论元}},]的格式回答。\\n\"\n",
      "                '答案：',\n",
      " 'label': '[{\"trigger\": \"抓获\", \"type\": \"司法行为-拘捕\", \"arguments\": {\"被拘捕者\": \"男子\", '\n",
      "          '\"拘捕者\": \"警方\"}}]'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_json_path = \"/root/autodl-tmp/ie/yayi_uie_sft_data/train_sft.jsonl\"\n",
    "\n",
    "test_json_path = \"/root/autodl-tmp/ie/yayi_uie_sft_data/test_sft.jsonl\"\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset_from_disk(json_path):\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=test_json_path, split=\"train\")\n",
    "# print(dataset)\n",
    "pprint(dataset[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d5d1fc-e923-4137-a113-40571a51b47c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:13:02.660816Z",
     "iopub.status.busy": "2024-07-11T05:13:02.660481Z",
     "iopub.status.idle": "2024-07-11T05:13:09.015506Z",
     "shell.execute_reply": "2024-07-11T05:13:09.014317Z",
     "shell.execute_reply.started": "2024-07-11T05:13:02.660788Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab0cde3f35c4863a4bf73f9dfb23481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"初始化一些参数\"\"\"\n",
    "# model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "# cache_dir = \"/root/autodl-tmp/qwen2-7b-instruct\"\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "cache_dir = \"/root/autodl-tmp/Qwen1.5-7B-Chat/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, trust_remote_code=True, padding=\"right\")\n",
    "max_length = 4098\n",
    "max_length = 1232 + 256\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4544c7e9-5db2-46f2-bbff-aa2b418c9999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:13:09.017074Z",
     "iopub.status.busy": "2024-07-11T05:13:09.016813Z",
     "iopub.status.idle": "2024-07-11T05:13:09.057517Z",
     "shell.execute_reply": "2024-07-11T05:13:09.056379Z",
     "shell.execute_reply.started": "2024-07-11T05:13:09.017045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [113921, 113129], 'attention_mask': [1, 1]} 开发者\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "我要退货<|im_end|>\n",
      "<|im_start|>system\n",
      "请提供单号<|im_end|>\n",
      "<|im_start|>user\n",
      "123456<|im_end|>\n",
      "<|im_start|>assistant\n",
      "已为您退货完成<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"测试tokenizer\"\"\"\n",
    "\n",
    "print(tokenizer(\"稀土开发者\"), tokenizer.decode([113129]))\n",
    "\n",
    "# one_example = dataset[0]\n",
    "# print(type(one_example))\n",
    "# pprint(one_example)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "测试对话模板\n",
    "\"\"\"\n",
    "prompt = \"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"assistant\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"我要退货\"},\n",
    "    {\"role\": \"system\", \"content\": \"请提供单号\"},\n",
    "    {\"role\": \"user\", \"content\": \"123456\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"已为您退货完成\"}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(text)\n",
    "\n",
    "print(tokenizer.chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a5d442-4492-4a9f-a37b-e14c34e3c73e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:13:16.004508Z",
     "iopub.status.busy": "2024-07-11T05:13:16.002571Z",
     "iopub.status.idle": "2024-07-11T05:13:22.330445Z",
     "shell.execute_reply": "2024-07-11T05:13:22.329404Z",
     "shell.execute_reply.started": "2024-07-11T05:13:16.004463Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8de9af7cc6541a5b7f9ef46ceec0671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5450\n",
      "})\n",
      "2\n",
      "打印前5个的长度\n",
      "1101 1101 1101\n",
      "1110 1110 1110\n",
      "1110 1110 1110\n",
      "1117 1117 1117\n",
      "1231 1231 1231\n",
      "1139 1139 1139\n",
      "1121 1121 1121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# print(one_example[\"instruction\"])\n",
    "\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "# 参考qwen2官方参考的finetuning代码，发现是错误的\n",
    "def process_fun_error(example: Dict):\n",
    "    \"\"\"自己实现instruction和response的区分\"\"\"\n",
    "    system_prompt = \"你是一个文本信息抽取领域的专家，现在需要你从用户user的给定文本中任务类型完成对应的信息抽取任务\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    response = example[\"label\"]\n",
    "    messages = [{\"role\": \"user\", \"content\": instruction}, {\"role\": \"system\", \"content\": labels}]\n",
    "    tokenized_message_ids = tokenizer.apply_chat_template(messages, \n",
    "                                                      tokenize=True, \n",
    "                                                      chat_template=tokenizer.chat_template,\n",
    "                                                      add_generation_prompt=False, \n",
    "                                                      padding=\"max_length\", \n",
    "                                                      max_length=8192, \n",
    "                                                      truncation=True)  # # list[int]\n",
    "    # print(tokenizer.decode(tokenized_message_ids))\n",
    "    input_ids = torch.tensor(tokenized_message_ids, dtype=torch.int)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[target_ids==tokenizer.pad_token_id] = LabelSmoother.ignore_index  # -100\n",
    "    attenion_mask = input_ids.ne(tokenizer.pad_token_id).int()   # boolean转int类型\n",
    "    return dict(input_ids=input_ids, attenion_mask=attenion_mask, target_ids=target_ids)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "对数据的预处理操作，根据不同的要求有不同的处理方法：\n",
    "1、根据instruction 和 response 分别进行tokenize化，然后进行组装，根据设置的最大长度进行阶段\n",
    "\n",
    "2、根据instruction 和 response 先进行拼接，然后tokeniz化，根据设置的最大长度进行拼接。（trl中的DataCollatorForOnlyCompletion就是这样操作的）\n",
    "\n",
    "\n",
    "\n",
    "上述两种方法本质上没有特殊的区别：但是在最大长度不能包括全部数据上时，就会出现response不全的情况，对于LLM大模型抽取的任务来说，就会产生不一致的结果（特别是要求输出为json的任务）\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def process_fun_qwen15(example):\n",
    "    \n",
    "    instruction = example[\"instruction\"]\n",
    "    response = example[\"label\"]\n",
    "    # messages = [{\"role\": \"user\", \"content\": instruction}, {\"role\": \"system\", \"content\": labels}]\n",
    "    # tokenized_message_ids = tokenizer.apply_chat_template(messages, \n",
    "    #                                                   tokenize=True, \n",
    "    #                                                   chat_template=tokenizer.chat_template,\n",
    "    #                                                   add_generation_prompt=False, \n",
    "    #                                                   padding=\"max_length\", \n",
    "    #                                                   max_length=8192, \n",
    "    #                                                   truncation=True)  # # list[int]\n",
    "    \n",
    "    template = f\"<|im_start|>system\\n你是一个文本信息抽取领域的专家，现在需要你从用户user的给定文本中任务类型完成对应的信息抽取任务<|im_end|>\\n<|im_start|>user\\n{instruction }<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    tokenized_instruction = tokenizer(template, add_special_tokens=False, truncation=True)\n",
    "    tokenized_response = tokenizer(response, add_special_tokens=False, truncation=True)\n",
    "    # 组装了输入和输出（这是根据id进行组装的，更好的处理方式是：）\n",
    "    input_ids = tokenized_instruction[\"input_ids\"] + tokenized_response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = tokenized_instruction[\"attention_mask\"] + tokenized_response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(tokenized_instruction[\"input_ids\"]) + tokenized_response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "# example_inputs = process_fun(one_example)\n",
    "# tt = example_inputs[\"target_ids\"]\n",
    "# print(\"target_padding的大小：\", tt.shape)\n",
    "# print(tt,tokenizer.pad_token_id, LabelSmoother.ignore_index)\n",
    "\n",
    "# ta = tt[tt != -100]\n",
    "# print(\"target的大小：\", ta.shape)\n",
    "# # print(tokenizer.decode(ta))\n",
    "\n",
    "    \n",
    "# ii = example_inputs[\"input_ids\"]\n",
    "# print(\"解码如下--------------------------：\\n\", tokenizer.decode(ii))\n",
    "\n",
    "def collate_pad_fn(examples):\n",
    "    return tokenizer.pad(examples, return_tensors=\"pt\", padding=True, max_length=max_length)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(process_fun_qwen15, remove_columns=dataset.column_names, batched=False)\n",
    "\n",
    "print(tokenized_dataset)\n",
    "\n",
    "print(len(tokenized_dataset[:2][\"input_ids\"]))   # 长度为2\n",
    "# print(tokenized_dataset[:2])\n",
    "print(\"打印前5个的长度\")\n",
    "\n",
    "for index, example in enumerate(tokenized_dataset):\n",
    "    if index > 6:\n",
    "        break\n",
    "    else:\n",
    "        print(len(example[\"input_ids\"]), len(example[\"attention_mask\"]), len(example[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13030cc9-2b5b-43d5-9f5e-f7923d3b1e03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:47:46.391788Z",
     "iopub.status.busy": "2024-07-11T05:47:46.390807Z",
     "iopub.status.idle": "2024-07-11T05:47:50.984305Z",
     "shell.execute_reply": "2024-07-11T05:47:50.982842Z",
     "shell.execute_reply.started": "2024-07-11T05:47:46.391710Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([151644,   8948,    198,  ...,  30975,     60, 151643]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]), 'labels': tensor([  -100,   -100,   -100,  ...,  30975,     60, 151643])}\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5450\n",
      "})\n",
      "max_length:  1488\n",
      "torch.Size([32, 1488])\n",
      "torch.Size([32, 744])\n",
      "torch.Size([32, 1197])\n",
      "torch.Size([32, 1488])\n",
      "torch.Size([32, 844])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 634])\n",
      "torch.Size([32, 1309])\n",
      "max_input_length:  1488\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "测试datasets.load_dataset产生的数据集和torch.utils.data.DataLoader的混合使用\n",
    "参考： https://huggingface.co/docs/datasets/use_with_pytorch\n",
    "\"\"\"\n",
    "\n",
    "# tokenized_dataset_torch = tokenized_dataset.with_format(\"torch\")\n",
    "# tokenized_dataset_torch = \n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "tokenized_dataset_torch = tokenized_dataset\n",
    "print(tokenized_dataset_torch[0])\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    "\n",
    "print(tokenized_dataset_torch)\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "\n",
    "# dataloader = DataLoader(tokenized_dataset_torch, batch_size=1, shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer,pad_to_multiple_of=8),)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True, max_length=1024)\n",
    "print(\"max_length: \", max_length)\n",
    "dataloader = DataLoader(tokenized_dataset_torch, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "\n",
    "max_input_length = 0\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    if batch[\"input_ids\"].shape[1] > max_input_length:\n",
    "        max_input_length = batch[\"input_ids\"].shape[1]\n",
    "    if i > 6:\n",
    "        # break\n",
    "        continue\n",
    "    else:\n",
    "        print(batch[\"input_ids\"].shape)\n",
    "\n",
    "print(\"max_input_length: \", max_input_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e28cd3-fa75-413a-845d-a346797cdadb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "batch_inputs ={\"input_ids\":torch.tensor([[1, 2], [3, 4, 5]]), \"attention_mask\": torch.tensor([[1,1], [1,1,1]])}\n",
    "\n",
    "print(tokenizer.pad(batch_inputs, padding=True, max_length=4))\n",
    "\n",
    "\n",
    "batch_inputs = [\"我去\", \"我去上海\", \"我要去上海了\", \"我明天去上海\", \"\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844070e-5af4-4d84-a38a-5d9a511ad8ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 定义Lora\n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                        inference_mode=False,lora_alpha=32, r=8, lora_dropout=0.1)\n",
    "\n",
    "\n",
    "# gradient_checkpoint=True, 必须设置: model.enable_input_require_grads()\n",
    "\n",
    "gradient_checkpointing = True\n",
    "if gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "model = get_peft_model(model=model, peft_config=lora_config)\n",
    "print(\"模型可训练参数：\", model.print_trainable_parameters())\n",
    "args = TrainingArguments(output_dir=\"/root/autodl-tmp/yayi_output/\",\n",
    "                         per_device_eval_batch_size=1, gradient_accumulation_steps=1, \n",
    "                         logging_dir=\"/root/autodl-tmp/yayi_output/logging\",\n",
    "                         logging_steps=1, \n",
    "                         learning_rate=3e-5, \n",
    "                         save_steps=50, save_total_limit=1,\n",
    "                         num_train_epochs=3, \n",
    "                         gradient_checkpointing=gradient_checkpointing)\n",
    "\n",
    "\n",
    "trainer = Trainer(model=model, args=args, \n",
    "                  data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True, label_pad_token_id=-100),\n",
    "                  train_dataset=tokenized_dataset)\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ecaf3d-061a-43c0-a9b8-35c7a6c184a2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T09:07:31.034864Z",
     "iopub.status.idle": "2024-07-09T09:07:31.035075Z",
     "shell.execute_reply": "2024-07-09T09:07:31.034976Z",
     "shell.execute_reply.started": "2024-07-09T09:07:31.034966Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# 参考： https://github.com/QwenLM/Qwen2/blob/main/examples/sft/finetune.py#L144\n",
    "# \"\"\"\n",
    "\n",
    "# from transformers import PreTrainedTokenizer\n",
    "# import transformers\n",
    "\n",
    "# TEMPLATE = \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content']}}{% if loop.last %}{{ '<|im_end|>'}}{% else %}{{ '<|im_end|>\\n' }}{% endif %}{% endfor %}\"\n",
    "\n",
    "\n",
    "\n",
    "# def preprocess(\n",
    "#     messages,\n",
    "#     tokenizer: transformers.PreTrainedTokenizer,\n",
    "#     max_len: int,\n",
    "# ) -> Dict:\n",
    "#     \"\"\"Preprocesses the data for supervised fine-tuning.\"\"\"\n",
    "\n",
    "#     texts = []\n",
    "#     for i, msg in enumerate(messages):\n",
    "#         texts.append(\n",
    "#             tokenizer.apply_chat_template(\n",
    "#                 msg,\n",
    "#                 chat_template=TEMPLATE,\n",
    "#                 tokenize=True,\n",
    "#                 add_generation_prompt=False,\n",
    "#                 padding=\"max_length\",\n",
    "#                 max_length=max_len,\n",
    "#                 truncation=True,\n",
    "#             )\n",
    "#         )\n",
    "#     input_ids = torch.tensor(texts, dtype=torch.int)\n",
    "#     target_ids = input_ids.clone()\n",
    "#     target_ids[target_ids == tokenizer.pad_token_id] = LabelSmoother.ignore_index\n",
    "#     attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "#     return dict(\n",
    "#         input_ids=input_ids, target_ids=target_ids, attention_mask=attention_mask\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # pprint(one_example)\n",
    "\n",
    "# one_messages = [{\"role\": \"user\", \"content\": one_example[\"instruction\"]}, {\"role\": \"system\", \"content\": one_example[\"label\"]}]\n",
    "# # print(one_messages)\n",
    "\n",
    "\n",
    "# ins = preprocess([one_messages], tokenizer, max_len=8192)\n",
    "\n",
    "# ins_ids = ins[\"input_ids\"][0]\n",
    "# # print(tokenizer.decode(ins_ids[ins_ids!=151643151643]))\n",
    "# print(ins[\"attention_mask\"][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
