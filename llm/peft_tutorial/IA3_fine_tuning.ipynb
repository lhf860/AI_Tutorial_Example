{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T07:47:39.947153Z",
     "iopub.status.busy": "2024-06-25T07:47:39.946379Z",
     "iopub.status.idle": "2024-06-25T07:47:44.002955Z",
     "shell.execute_reply": "2024-06-25T07:47:44.000937Z",
     "shell.execute_reply.started": "2024-06-25T07:47:39.947063Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "\n",
    "cache_dir = \"/root/autodl-tmp\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PrefixEncoder, PrefixTuningConfig\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T07:47:44.005457Z",
     "iopub.status.busy": "2024-06-25T07:47:44.005047Z",
     "iopub.status.idle": "2024-06-25T07:47:44.027692Z",
     "shell.execute_reply": "2024-06-25T07:47:44.025766Z",
     "shell.execute_reply.started": "2024-06-25T07:47:44.005427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。'],\n",
       " 'input': [''],\n",
       " 'instruction': ['保持健康的三个提示。']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "ds = Dataset.load_from_disk(\"./alpaca_data_zh\")\n",
    "ds[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T07:47:44.028817Z",
     "iopub.status.busy": "2024-06-25T07:47:44.028541Z",
     "iopub.status.idle": "2024-06-25T07:47:44.678581Z",
     "shell.execute_reply": "2024-06-25T07:47:44.676654Z",
     "shell.execute_reply.started": "2024-06-25T07:47:44.028789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='Langboat/bloom-1b4-zh', vocab_size=46145, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"Langboat/bloom-1b4-zh\", cache_dir=os.path.join(cache_dir, \"bloom-1b4-zh\"))\n",
    "\n",
    "tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T07:47:44.680449Z",
     "iopub.status.busy": "2024-06-25T07:47:44.680171Z",
     "iopub.status.idle": "2024-06-25T07:47:44.691958Z",
     "shell.execute_reply": "2024-06-25T07:47:44.689876Z",
     "shell.execute_reply.started": "2024-06-25T07:47:44.680420Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# def process_func(example):\n",
    "#     input_ids, attention_mask, labels =[], [], []\n",
    "#     instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"].strip() , \"\\n\\nAssistant: \"]))\n",
    "#     response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "#     input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "#     attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "#     labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "\n",
    "#     max_length = 256\n",
    "#     if len(input_ids) > max_length:\n",
    "#         input_ids = input_ids[:max_length]\n",
    "#         attention_mask = attention_mask[:max_length]\n",
    "#         labels = labels[: max_length]\n",
    "#     return {\"input_ids\": input_ids, \"attention_mask\":attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T07:47:44.694345Z",
     "iopub.status.busy": "2024-06-25T07:47:44.693943Z",
     "iopub.status.idle": "2024-06-25T07:47:44.756257Z",
     "shell.execute_reply": "2024-06-25T07:47:44.754136Z",
     "shell.execute_reply.started": "2024-06-25T07:47:44.694307Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [26283,\n",
       "  29,\n",
       "  210,\n",
       "  7160,\n",
       "  28215,\n",
       "  9905,\n",
       "  15211,\n",
       "  672,\n",
       "  189,\n",
       "  4340,\n",
       "  17245,\n",
       "  29,\n",
       "  210,\n",
       "  5040,\n",
       "  584,\n",
       "  7160,\n",
       "  28215,\n",
       "  9905,\n",
       "  15211,\n",
       "  1038,\n",
       "  189,\n",
       "  189,\n",
       "  20,\n",
       "  17,\n",
       "  210,\n",
       "  7160,\n",
       "  9239,\n",
       "  3365,\n",
       "  420,\n",
       "  9487,\n",
       "  1965,\n",
       "  15342,\n",
       "  9239,\n",
       "  6053,\n",
       "  355,\n",
       "  1227,\n",
       "  32998,\n",
       "  554,\n",
       "  34487,\n",
       "  1326,\n",
       "  17632,\n",
       "  355,\n",
       "  975,\n",
       "  5189,\n",
       "  39177,\n",
       "  6455,\n",
       "  355,\n",
       "  12144,\n",
       "  17335,\n",
       "  8481,\n",
       "  355,\n",
       "  1437,\n",
       "  12785,\n",
       "  6698,\n",
       "  23294,\n",
       "  672,\n",
       "  189,\n",
       "  21,\n",
       "  17,\n",
       "  210,\n",
       "  22905,\n",
       "  15356,\n",
       "  420,\n",
       "  9487,\n",
       "  15597,\n",
       "  22985,\n",
       "  373,\n",
       "  19320,\n",
       "  554,\n",
       "  18524,\n",
       "  554,\n",
       "  1229,\n",
       "  6663,\n",
       "  1467,\n",
       "  642,\n",
       "  16422,\n",
       "  18235,\n",
       "  27624,\n",
       "  17520,\n",
       "  8732,\n",
       "  355,\n",
       "  8729,\n",
       "  1246,\n",
       "  7110,\n",
       "  554,\n",
       "  1246,\n",
       "  16422,\n",
       "  642,\n",
       "  15224,\n",
       "  11367,\n",
       "  355,\n",
       "  718,\n",
       "  7160,\n",
       "  28215,\n",
       "  15356,\n",
       "  12577,\n",
       "  672,\n",
       "  189,\n",
       "  22,\n",
       "  17,\n",
       "  210,\n",
       "  20027,\n",
       "  18399,\n",
       "  420,\n",
       "  20027,\n",
       "  1079,\n",
       "  15713,\n",
       "  6455,\n",
       "  21230,\n",
       "  355,\n",
       "  34810,\n",
       "  9487,\n",
       "  1688,\n",
       "  8637,\n",
       "  967,\n",
       "  3958,\n",
       "  210,\n",
       "  43738,\n",
       "  20027,\n",
       "  420,\n",
       "  15822,\n",
       "  20027,\n",
       "  12785,\n",
       "  17374,\n",
       "  12115,\n",
       "  355,\n",
       "  5189,\n",
       "  9239,\n",
       "  8113,\n",
       "  355,\n",
       "  1437,\n",
       "  5706,\n",
       "  4202,\n",
       "  19112,\n",
       "  16610,\n",
       "  1276,\n",
       "  420,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  5040,\n",
       "  584,\n",
       "  7160,\n",
       "  28215,\n",
       "  9905,\n",
       "  15211,\n",
       "  1038,\n",
       "  189,\n",
       "  189,\n",
       "  20,\n",
       "  17,\n",
       "  210,\n",
       "  7160,\n",
       "  9239,\n",
       "  3365,\n",
       "  420,\n",
       "  9487,\n",
       "  1965,\n",
       "  15342,\n",
       "  9239,\n",
       "  6053,\n",
       "  355,\n",
       "  1227,\n",
       "  32998,\n",
       "  554,\n",
       "  34487,\n",
       "  1326,\n",
       "  17632,\n",
       "  355,\n",
       "  975,\n",
       "  5189,\n",
       "  39177,\n",
       "  6455,\n",
       "  355,\n",
       "  12144,\n",
       "  17335,\n",
       "  8481,\n",
       "  355,\n",
       "  1437,\n",
       "  12785,\n",
       "  6698,\n",
       "  23294,\n",
       "  672,\n",
       "  189,\n",
       "  21,\n",
       "  17,\n",
       "  210,\n",
       "  22905,\n",
       "  15356,\n",
       "  420,\n",
       "  9487,\n",
       "  15597,\n",
       "  22985,\n",
       "  373,\n",
       "  19320,\n",
       "  554,\n",
       "  18524,\n",
       "  554,\n",
       "  1229,\n",
       "  6663,\n",
       "  1467,\n",
       "  642,\n",
       "  16422,\n",
       "  18235,\n",
       "  27624,\n",
       "  17520,\n",
       "  8732,\n",
       "  355,\n",
       "  8729,\n",
       "  1246,\n",
       "  7110,\n",
       "  554,\n",
       "  1246,\n",
       "  16422,\n",
       "  642,\n",
       "  15224,\n",
       "  11367,\n",
       "  355,\n",
       "  718,\n",
       "  7160,\n",
       "  28215,\n",
       "  15356,\n",
       "  12577,\n",
       "  672,\n",
       "  189,\n",
       "  22,\n",
       "  17,\n",
       "  210,\n",
       "  20027,\n",
       "  18399,\n",
       "  420,\n",
       "  20027,\n",
       "  1079,\n",
       "  15713,\n",
       "  6455,\n",
       "  21230,\n",
       "  355,\n",
       "  34810,\n",
       "  9487,\n",
       "  1688,\n",
       "  8637,\n",
       "  967,\n",
       "  3958,\n",
       "  210,\n",
       "  43738,\n",
       "  20027,\n",
       "  420,\n",
       "  15822,\n",
       "  20027,\n",
       "  12785,\n",
       "  17374,\n",
       "  12115,\n",
       "  355,\n",
       "  5189,\n",
       "  9239,\n",
       "  8113,\n",
       "  355,\n",
       "  1437,\n",
       "  5706,\n",
       "  4202,\n",
       "  19112,\n",
       "  16610,\n",
       "  1276,\n",
       "  420,\n",
       "  2]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "\n",
    "tokenized_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T07:47:45.512699Z",
     "iopub.status.busy": "2024-06-25T07:47:45.512241Z",
     "iopub.status.idle": "2024-06-25T07:47:46.362779Z",
     "shell.execute_reply": "2024-06-25T07:47:46.360634Z",
     "shell.execute_reply.started": "2024-06-25T07:47:45.512660Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 创建模型\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-1b4-zh\", low_cpu_mem_usage=True, cache_dir=os.path.join(cache_dir, \"bloom-1b4-zh\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T07:47:48.694731Z",
     "iopub.status.busy": "2024-06-25T07:47:48.692411Z",
     "iopub.status.idle": "2024-06-25T07:47:48.712071Z",
     "shell.execute_reply": "2024-06-25T07:47:48.709539Z",
     "shell.execute_reply.started": "2024-06-25T07:47:48.694627Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight\n",
      "transformer.word_embeddings_layernorm.weight\n",
      "transformer.word_embeddings_layernorm.bias\n",
      "transformer.h.0.input_layernorm.weight\n",
      "transformer.h.0.input_layernorm.bias\n",
      "transformer.h.0.self_attention.query_key_value.weight\n",
      "transformer.h.0.self_attention.query_key_value.bias\n",
      "transformer.h.0.self_attention.dense.weight\n",
      "transformer.h.0.self_attention.dense.bias\n",
      "transformer.h.0.post_attention_layernorm.weight\n",
      "transformer.h.0.post_attention_layernorm.bias\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias\n",
      "transformer.h.1.input_layernorm.weight\n",
      "transformer.h.1.input_layernorm.bias\n",
      "transformer.h.1.self_attention.query_key_value.weight\n",
      "transformer.h.1.self_attention.query_key_value.bias\n",
      "transformer.h.1.self_attention.dense.weight\n",
      "transformer.h.1.self_attention.dense.bias\n",
      "transformer.h.1.post_attention_layernorm.weight\n",
      "transformer.h.1.post_attention_layernorm.bias\n",
      "transformer.h.1.mlp.dense_h_to_4h.weight\n",
      "transformer.h.1.mlp.dense_h_to_4h.bias\n",
      "transformer.h.1.mlp.dense_4h_to_h.weight\n",
      "transformer.h.1.mlp.dense_4h_to_h.bias\n",
      "transformer.h.2.input_layernorm.weight\n",
      "transformer.h.2.input_layernorm.bias\n",
      "transformer.h.2.self_attention.query_key_value.weight\n",
      "transformer.h.2.self_attention.query_key_value.bias\n",
      "transformer.h.2.self_attention.dense.weight\n",
      "transformer.h.2.self_attention.dense.bias\n",
      "transformer.h.2.post_attention_layernorm.weight\n",
      "transformer.h.2.post_attention_layernorm.bias\n",
      "transformer.h.2.mlp.dense_h_to_4h.weight\n",
      "transformer.h.2.mlp.dense_h_to_4h.bias\n",
      "transformer.h.2.mlp.dense_4h_to_h.weight\n",
      "transformer.h.2.mlp.dense_4h_to_h.bias\n",
      "transformer.h.3.input_layernorm.weight\n",
      "transformer.h.3.input_layernorm.bias\n",
      "transformer.h.3.self_attention.query_key_value.weight\n",
      "transformer.h.3.self_attention.query_key_value.bias\n",
      "transformer.h.3.self_attention.dense.weight\n",
      "transformer.h.3.self_attention.dense.bias\n",
      "transformer.h.3.post_attention_layernorm.weight\n",
      "transformer.h.3.post_attention_layernorm.bias\n",
      "transformer.h.3.mlp.dense_h_to_4h.weight\n",
      "transformer.h.3.mlp.dense_h_to_4h.bias\n",
      "transformer.h.3.mlp.dense_4h_to_h.weight\n",
      "transformer.h.3.mlp.dense_4h_to_h.bias\n",
      "transformer.h.4.input_layernorm.weight\n",
      "transformer.h.4.input_layernorm.bias\n",
      "transformer.h.4.self_attention.query_key_value.weight\n",
      "transformer.h.4.self_attention.query_key_value.bias\n",
      "transformer.h.4.self_attention.dense.weight\n",
      "transformer.h.4.self_attention.dense.bias\n",
      "transformer.h.4.post_attention_layernorm.weight\n",
      "transformer.h.4.post_attention_layernorm.bias\n",
      "transformer.h.4.mlp.dense_h_to_4h.weight\n",
      "transformer.h.4.mlp.dense_h_to_4h.bias\n",
      "transformer.h.4.mlp.dense_4h_to_h.weight\n",
      "transformer.h.4.mlp.dense_4h_to_h.bias\n",
      "transformer.h.5.input_layernorm.weight\n",
      "transformer.h.5.input_layernorm.bias\n",
      "transformer.h.5.self_attention.query_key_value.weight\n",
      "transformer.h.5.self_attention.query_key_value.bias\n",
      "transformer.h.5.self_attention.dense.weight\n",
      "transformer.h.5.self_attention.dense.bias\n",
      "transformer.h.5.post_attention_layernorm.weight\n",
      "transformer.h.5.post_attention_layernorm.bias\n",
      "transformer.h.5.mlp.dense_h_to_4h.weight\n",
      "transformer.h.5.mlp.dense_h_to_4h.bias\n",
      "transformer.h.5.mlp.dense_4h_to_h.weight\n",
      "transformer.h.5.mlp.dense_4h_to_h.bias\n",
      "transformer.h.6.input_layernorm.weight\n",
      "transformer.h.6.input_layernorm.bias\n",
      "transformer.h.6.self_attention.query_key_value.weight\n",
      "transformer.h.6.self_attention.query_key_value.bias\n",
      "transformer.h.6.self_attention.dense.weight\n",
      "transformer.h.6.self_attention.dense.bias\n",
      "transformer.h.6.post_attention_layernorm.weight\n",
      "transformer.h.6.post_attention_layernorm.bias\n",
      "transformer.h.6.mlp.dense_h_to_4h.weight\n",
      "transformer.h.6.mlp.dense_h_to_4h.bias\n",
      "transformer.h.6.mlp.dense_4h_to_h.weight\n",
      "transformer.h.6.mlp.dense_4h_to_h.bias\n",
      "transformer.h.7.input_layernorm.weight\n",
      "transformer.h.7.input_layernorm.bias\n",
      "transformer.h.7.self_attention.query_key_value.weight\n",
      "transformer.h.7.self_attention.query_key_value.bias\n",
      "transformer.h.7.self_attention.dense.weight\n",
      "transformer.h.7.self_attention.dense.bias\n",
      "transformer.h.7.post_attention_layernorm.weight\n",
      "transformer.h.7.post_attention_layernorm.bias\n",
      "transformer.h.7.mlp.dense_h_to_4h.weight\n",
      "transformer.h.7.mlp.dense_h_to_4h.bias\n",
      "transformer.h.7.mlp.dense_4h_to_h.weight\n",
      "transformer.h.7.mlp.dense_4h_to_h.bias\n",
      "transformer.h.8.input_layernorm.weight\n",
      "transformer.h.8.input_layernorm.bias\n",
      "transformer.h.8.self_attention.query_key_value.weight\n",
      "transformer.h.8.self_attention.query_key_value.bias\n",
      "transformer.h.8.self_attention.dense.weight\n",
      "transformer.h.8.self_attention.dense.bias\n",
      "transformer.h.8.post_attention_layernorm.weight\n",
      "transformer.h.8.post_attention_layernorm.bias\n",
      "transformer.h.8.mlp.dense_h_to_4h.weight\n",
      "transformer.h.8.mlp.dense_h_to_4h.bias\n",
      "transformer.h.8.mlp.dense_4h_to_h.weight\n",
      "transformer.h.8.mlp.dense_4h_to_h.bias\n",
      "transformer.h.9.input_layernorm.weight\n",
      "transformer.h.9.input_layernorm.bias\n",
      "transformer.h.9.self_attention.query_key_value.weight\n",
      "transformer.h.9.self_attention.query_key_value.bias\n",
      "transformer.h.9.self_attention.dense.weight\n",
      "transformer.h.9.self_attention.dense.bias\n",
      "transformer.h.9.post_attention_layernorm.weight\n",
      "transformer.h.9.post_attention_layernorm.bias\n",
      "transformer.h.9.mlp.dense_h_to_4h.weight\n",
      "transformer.h.9.mlp.dense_h_to_4h.bias\n",
      "transformer.h.9.mlp.dense_4h_to_h.weight\n",
      "transformer.h.9.mlp.dense_4h_to_h.bias\n",
      "transformer.h.10.input_layernorm.weight\n",
      "transformer.h.10.input_layernorm.bias\n",
      "transformer.h.10.self_attention.query_key_value.weight\n",
      "transformer.h.10.self_attention.query_key_value.bias\n",
      "transformer.h.10.self_attention.dense.weight\n",
      "transformer.h.10.self_attention.dense.bias\n",
      "transformer.h.10.post_attention_layernorm.weight\n",
      "transformer.h.10.post_attention_layernorm.bias\n",
      "transformer.h.10.mlp.dense_h_to_4h.weight\n",
      "transformer.h.10.mlp.dense_h_to_4h.bias\n",
      "transformer.h.10.mlp.dense_4h_to_h.weight\n",
      "transformer.h.10.mlp.dense_4h_to_h.bias\n",
      "transformer.h.11.input_layernorm.weight\n",
      "transformer.h.11.input_layernorm.bias\n",
      "transformer.h.11.self_attention.query_key_value.weight\n",
      "transformer.h.11.self_attention.query_key_value.bias\n",
      "transformer.h.11.self_attention.dense.weight\n",
      "transformer.h.11.self_attention.dense.bias\n",
      "transformer.h.11.post_attention_layernorm.weight\n",
      "transformer.h.11.post_attention_layernorm.bias\n",
      "transformer.h.11.mlp.dense_h_to_4h.weight\n",
      "transformer.h.11.mlp.dense_h_to_4h.bias\n",
      "transformer.h.11.mlp.dense_4h_to_h.weight\n",
      "transformer.h.11.mlp.dense_4h_to_h.bias\n",
      "transformer.h.12.input_layernorm.weight\n",
      "transformer.h.12.input_layernorm.bias\n",
      "transformer.h.12.self_attention.query_key_value.weight\n",
      "transformer.h.12.self_attention.query_key_value.bias\n",
      "transformer.h.12.self_attention.dense.weight\n",
      "transformer.h.12.self_attention.dense.bias\n",
      "transformer.h.12.post_attention_layernorm.weight\n",
      "transformer.h.12.post_attention_layernorm.bias\n",
      "transformer.h.12.mlp.dense_h_to_4h.weight\n",
      "transformer.h.12.mlp.dense_h_to_4h.bias\n",
      "transformer.h.12.mlp.dense_4h_to_h.weight\n",
      "transformer.h.12.mlp.dense_4h_to_h.bias\n",
      "transformer.h.13.input_layernorm.weight\n",
      "transformer.h.13.input_layernorm.bias\n",
      "transformer.h.13.self_attention.query_key_value.weight\n",
      "transformer.h.13.self_attention.query_key_value.bias\n",
      "transformer.h.13.self_attention.dense.weight\n",
      "transformer.h.13.self_attention.dense.bias\n",
      "transformer.h.13.post_attention_layernorm.weight\n",
      "transformer.h.13.post_attention_layernorm.bias\n",
      "transformer.h.13.mlp.dense_h_to_4h.weight\n",
      "transformer.h.13.mlp.dense_h_to_4h.bias\n",
      "transformer.h.13.mlp.dense_4h_to_h.weight\n",
      "transformer.h.13.mlp.dense_4h_to_h.bias\n",
      "transformer.h.14.input_layernorm.weight\n",
      "transformer.h.14.input_layernorm.bias\n",
      "transformer.h.14.self_attention.query_key_value.weight\n",
      "transformer.h.14.self_attention.query_key_value.bias\n",
      "transformer.h.14.self_attention.dense.weight\n",
      "transformer.h.14.self_attention.dense.bias\n",
      "transformer.h.14.post_attention_layernorm.weight\n",
      "transformer.h.14.post_attention_layernorm.bias\n",
      "transformer.h.14.mlp.dense_h_to_4h.weight\n",
      "transformer.h.14.mlp.dense_h_to_4h.bias\n",
      "transformer.h.14.mlp.dense_4h_to_h.weight\n",
      "transformer.h.14.mlp.dense_4h_to_h.bias\n",
      "transformer.h.15.input_layernorm.weight\n",
      "transformer.h.15.input_layernorm.bias\n",
      "transformer.h.15.self_attention.query_key_value.weight\n",
      "transformer.h.15.self_attention.query_key_value.bias\n",
      "transformer.h.15.self_attention.dense.weight\n",
      "transformer.h.15.self_attention.dense.bias\n",
      "transformer.h.15.post_attention_layernorm.weight\n",
      "transformer.h.15.post_attention_layernorm.bias\n",
      "transformer.h.15.mlp.dense_h_to_4h.weight\n",
      "transformer.h.15.mlp.dense_h_to_4h.bias\n",
      "transformer.h.15.mlp.dense_4h_to_h.weight\n",
      "transformer.h.15.mlp.dense_4h_to_h.bias\n",
      "transformer.h.16.input_layernorm.weight\n",
      "transformer.h.16.input_layernorm.bias\n",
      "transformer.h.16.self_attention.query_key_value.weight\n",
      "transformer.h.16.self_attention.query_key_value.bias\n",
      "transformer.h.16.self_attention.dense.weight\n",
      "transformer.h.16.self_attention.dense.bias\n",
      "transformer.h.16.post_attention_layernorm.weight\n",
      "transformer.h.16.post_attention_layernorm.bias\n",
      "transformer.h.16.mlp.dense_h_to_4h.weight\n",
      "transformer.h.16.mlp.dense_h_to_4h.bias\n",
      "transformer.h.16.mlp.dense_4h_to_h.weight\n",
      "transformer.h.16.mlp.dense_4h_to_h.bias\n",
      "transformer.h.17.input_layernorm.weight\n",
      "transformer.h.17.input_layernorm.bias\n",
      "transformer.h.17.self_attention.query_key_value.weight\n",
      "transformer.h.17.self_attention.query_key_value.bias\n",
      "transformer.h.17.self_attention.dense.weight\n",
      "transformer.h.17.self_attention.dense.bias\n",
      "transformer.h.17.post_attention_layernorm.weight\n",
      "transformer.h.17.post_attention_layernorm.bias\n",
      "transformer.h.17.mlp.dense_h_to_4h.weight\n",
      "transformer.h.17.mlp.dense_h_to_4h.bias\n",
      "transformer.h.17.mlp.dense_4h_to_h.weight\n",
      "transformer.h.17.mlp.dense_4h_to_h.bias\n",
      "transformer.h.18.input_layernorm.weight\n",
      "transformer.h.18.input_layernorm.bias\n",
      "transformer.h.18.self_attention.query_key_value.weight\n",
      "transformer.h.18.self_attention.query_key_value.bias\n",
      "transformer.h.18.self_attention.dense.weight\n",
      "transformer.h.18.self_attention.dense.bias\n",
      "transformer.h.18.post_attention_layernorm.weight\n",
      "transformer.h.18.post_attention_layernorm.bias\n",
      "transformer.h.18.mlp.dense_h_to_4h.weight\n",
      "transformer.h.18.mlp.dense_h_to_4h.bias\n",
      "transformer.h.18.mlp.dense_4h_to_h.weight\n",
      "transformer.h.18.mlp.dense_4h_to_h.bias\n",
      "transformer.h.19.input_layernorm.weight\n",
      "transformer.h.19.input_layernorm.bias\n",
      "transformer.h.19.self_attention.query_key_value.weight\n",
      "transformer.h.19.self_attention.query_key_value.bias\n",
      "transformer.h.19.self_attention.dense.weight\n",
      "transformer.h.19.self_attention.dense.bias\n",
      "transformer.h.19.post_attention_layernorm.weight\n",
      "transformer.h.19.post_attention_layernorm.bias\n",
      "transformer.h.19.mlp.dense_h_to_4h.weight\n",
      "transformer.h.19.mlp.dense_h_to_4h.bias\n",
      "transformer.h.19.mlp.dense_4h_to_h.weight\n",
      "transformer.h.19.mlp.dense_4h_to_h.bias\n",
      "transformer.h.20.input_layernorm.weight\n",
      "transformer.h.20.input_layernorm.bias\n",
      "transformer.h.20.self_attention.query_key_value.weight\n",
      "transformer.h.20.self_attention.query_key_value.bias\n",
      "transformer.h.20.self_attention.dense.weight\n",
      "transformer.h.20.self_attention.dense.bias\n",
      "transformer.h.20.post_attention_layernorm.weight\n",
      "transformer.h.20.post_attention_layernorm.bias\n",
      "transformer.h.20.mlp.dense_h_to_4h.weight\n",
      "transformer.h.20.mlp.dense_h_to_4h.bias\n",
      "transformer.h.20.mlp.dense_4h_to_h.weight\n",
      "transformer.h.20.mlp.dense_4h_to_h.bias\n",
      "transformer.h.21.input_layernorm.weight\n",
      "transformer.h.21.input_layernorm.bias\n",
      "transformer.h.21.self_attention.query_key_value.weight\n",
      "transformer.h.21.self_attention.query_key_value.bias\n",
      "transformer.h.21.self_attention.dense.weight\n",
      "transformer.h.21.self_attention.dense.bias\n",
      "transformer.h.21.post_attention_layernorm.weight\n",
      "transformer.h.21.post_attention_layernorm.bias\n",
      "transformer.h.21.mlp.dense_h_to_4h.weight\n",
      "transformer.h.21.mlp.dense_h_to_4h.bias\n",
      "transformer.h.21.mlp.dense_4h_to_h.weight\n",
      "transformer.h.21.mlp.dense_4h_to_h.bias\n",
      "transformer.h.22.input_layernorm.weight\n",
      "transformer.h.22.input_layernorm.bias\n",
      "transformer.h.22.self_attention.query_key_value.weight\n",
      "transformer.h.22.self_attention.query_key_value.bias\n",
      "transformer.h.22.self_attention.dense.weight\n",
      "transformer.h.22.self_attention.dense.bias\n",
      "transformer.h.22.post_attention_layernorm.weight\n",
      "transformer.h.22.post_attention_layernorm.bias\n",
      "transformer.h.22.mlp.dense_h_to_4h.weight\n",
      "transformer.h.22.mlp.dense_h_to_4h.bias\n",
      "transformer.h.22.mlp.dense_4h_to_h.weight\n",
      "transformer.h.22.mlp.dense_4h_to_h.bias\n",
      "transformer.h.23.input_layernorm.weight\n",
      "transformer.h.23.input_layernorm.bias\n",
      "transformer.h.23.self_attention.query_key_value.weight\n",
      "transformer.h.23.self_attention.query_key_value.bias\n",
      "transformer.h.23.self_attention.dense.weight\n",
      "transformer.h.23.self_attention.dense.bias\n",
      "transformer.h.23.post_attention_layernorm.weight\n",
      "transformer.h.23.post_attention_layernorm.bias\n",
      "transformer.h.23.mlp.dense_h_to_4h.weight\n",
      "transformer.h.23.mlp.dense_h_to_4h.bias\n",
      "transformer.h.23.mlp.dense_4h_to_h.weight\n",
      "transformer.h.23.mlp.dense_4h_to_h.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T07:56:42.205995Z",
     "iopub.status.busy": "2024-06-25T07:56:42.205029Z",
     "iopub.status.idle": "2024-06-25T07:56:42.219706Z",
     "shell.execute_reply": "2024-06-25T07:56:42.218185Z",
     "shell.execute_reply.started": "2024-06-25T07:56:42.205921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IA3Config(peft_type=<PeftType.IA3: 'IA3'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, target_modules={'mlp.dense_4h_to_h', 'query_key_value'}, feedforward_modules={'mlp.dense_4h_to_h'}, fan_in_fan_out=False, modules_to_save=None, init_ia3_weights=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "创建lora微调的代码和结构\n",
    "\"\"\"\n",
    "\n",
    "# from peft import LoraConfig, TaskType, get_peft_model\n",
    "from peft import IA3Config, TaskType, get_peft_model\n",
    "\n",
    "config = IA3Config(task_type=TaskType.CAUSAL_LM,\n",
    "                   # 这是针对bloom模型设定好的target_modules,当执行get_peft_model时可以自动获得得到\n",
    "                   target_modules=[\"query_key_value\", \"mlp.dense_4h_to_h\"],  # 支持正则表达式\n",
    "                   feedforward_modules=[\"mlp.dense_4h_to_h\"],\n",
    "                   modules_to_save=None)\n",
    "\n",
    "config\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T07:57:04.978331Z",
     "iopub.status.busy": "2024-06-25T07:57:04.977475Z",
     "iopub.status.idle": "2024-06-25T07:57:13.973115Z",
     "shell.execute_reply": "2024-06-25T07:57:13.969962Z",
     "shell.execute_reply.started": "2024-06-25T07:57:04.978258Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 238.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 160.25 MiB is free. Process 187450 has 12.87 GiB memory in use. Process 192518 has 10.61 GiB memory in use. Of the allocated memory 9.67 GiB is allocated by PyTorch, and 542.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m get_peft_model(model\u001b[38;5;241m=\u001b[39mmodel, peft_config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mpeft_model,args \u001b[38;5;241m=\u001b[39m args,\n\u001b[1;32m     15\u001b[0m                   data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForSeq2Seq(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     16\u001b[0m                   train_dataset\u001b[38;5;241m=\u001b[39mtokenized_ds)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/peft_model.py:1430\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1429\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:873\u001b[0m, in \u001b[0;36mBloomForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m--> 873\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    878\u001b[0m     output \u001b[38;5;241m=\u001b[39m (lm_logits,) \u001b[38;5;241m+\u001b[39m transformer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 238.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 160.25 MiB is free. Process 187450 has 12.87 GiB memory in use. Process 192518 has 10.61 GiB memory in use. Of the allocated memory 9.67 GiB is allocated by PyTorch, and 542.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 创建训练参数\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "args = TrainingArguments(output_dir=\"/root/autodl-tmp/lora_fine_tuning_model\",\n",
    "                         per_device_train_batch_size=8,\n",
    "                         gradient_accumulation_steps=4,\n",
    "                         learning_rate=8e-5, \n",
    "                         logging_steps=10, save_total_limit=2, save_steps=50,\n",
    "                         num_train_epochs=2)\n",
    "\n",
    "peft_model = get_peft_model(model=model, peft_config=config)\n",
    "\n",
    "trainer = Trainer(model=peft_model,args = args,\n",
    "                  data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "                  train_dataset=tokenized_ds)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T04:37:45.833945Z",
     "iopub.status.busy": "2024-06-25T04:37:45.833081Z",
     "iopub.status.idle": "2024-06-25T04:37:47.050678Z",
     "shell.execute_reply": "2024-06-25T04:37:47.049842Z",
     "shell.execute_reply.started": "2024-06-25T04:37:45.833866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: 考试有哪些技巧？\\n\\nAssistant: 考试有很多技巧，其中最关键的技巧之一是保持良好的心态。保持良好的心态不仅有助于提高考试成绩，而且有助于保持积极的心态，避免紧张和焦虑。此外，考试技巧还包括合理安排时间，合理分配精力，保持良好的作息习惯，以及合理饮食和锻炼身体。此外，考试技巧还包括保持良好的学习习惯，如按时完成作业，认真听课，及时复习等。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"考试有哪些技巧？\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(model.device)\n",
    "tokenizer.decode(peft_model.generate(**ipt, max_length=128, do_sample=False)[0], skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
