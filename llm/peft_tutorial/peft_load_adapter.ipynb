{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85fde8e-1022-4f5c-8713-728f9799143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d8b0e4-67ce-4cfe-8076-e7b3e86c9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM, PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee0ef7f-5ae6-492d-b163-9f1292ae3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"Langboat/bloom-1b4-zh\"  # 原始模型model id\n",
    "peft_model_id = \"/root/autodl-tmp/prompt_tuning_bloom_1b4/checkpoint-1000/\"  # 经过微调后模型的加载和使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5cc9bb-ae3f-46b7-ac38-8f43c653df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = PeftConfig.from_pretrained(\"/root/autodl-tmp/prompt_tuning_bloom_1b4/save_pretrained\")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, cache_dir=\"/root/autodl-tmp/bloom-1b4-zh\")\n",
    "\n",
    "# 默认是推理模式，即is_trainable=False\n",
    "prompt_tuning_model = PeftModel.from_pretrained(model=model,model_id=peft_model_id,is_trainable=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985dc5c5-ed4c-4d5e-a37a-16a3629a3067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): BloomForCausalLM(\n",
       "    (transformer): BloomModel(\n",
       "      (word_embeddings): Embedding(46145, 2048)\n",
       "      (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x BloomBlock(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BloomAttention(\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BloomMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (gelu_impl): BloomGelu()\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=46145, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEmbedding(\n",
       "      (embedding): Embedding(8, 2048)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(46145, 2048)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt_tuning_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2acdf5f-dab9-4b5c-8a1c-6f1b2f02d208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt_tuning_model = prompt_tuning_model.cuda()\n",
    "prompt_tuning_model.device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "308b9c89-ccc9-44f2-832b-f92e183705bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "cache_dir = '/root/autodl-tmp/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-1b4-zh\", cache_dir=os.path.join(cache_dir, \"bloom-1b4-zh\"))\n",
    "\n",
    "\n",
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"考试有哪些好的技巧\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors='pt').to(prompt_tuning_model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad5976e8-dcd5-494a-bd72-116a026c9a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26283,    29,   210, 12913, 26620,  4775, 16012,   189,   189,  4340,\n",
       "         17245,    29,   210, 18244, 17799,   584,  7189, 13398,  4775,  4118,\n",
       "           355, 31212,  2775, 26656,   373,  8583,  7160, 21861,   355,  4573,\n",
       "          7189, 20677,   420, 11956,  2867, 16610,   554, 10849,   642, 10849,\n",
       "          1058,  6238,   865, 13165, 27431,   642, 10849, 16012,   355,  1058,\n",
       "          5706,  4851, 10849,  4155,   420, 10849,  4155,  5574,  6238,  1689,\n",
       "         24073, 27431,   355,   587, 17799, 10279,  5574,  4573, 27431,   355,\n",
       "         18516,  5706,  1689, 10849,   373, 12230,   420,  1689, 16645, 17799,\n",
       "         32299,   355,  5706,  4851, 32299,  8620,   355,  6238,  1689, 14383,\n",
       "           877,  8715,  1058, 14335,   642,  8971,   355,  3985,  7064, 16430,\n",
       "          7189,   655, 11293,   620, 16976, 26428,   672, 25078,  6349,  1700,\n",
       "         17799, 10849,   355, 16645, 17799, 22585,   355, 22585,  5500, 10849,\n",
       "         36159,  6238,   355,  5574,  6238,  1689, 16976,  9569,  8033,   355,\n",
       "         13165,   882,  6992, 22585, 14150,   355,   587, 22585,  6174,  5574,\n",
       "         14747, 11478, 18709, 14335, 24280,   355,   587, 12942,  4950,  6615,\n",
       "          6174,   355, 18516, 21838,  4851, 21219,  4118,   355,  3985,  7064,\n",
       "         16430,  1965,  1750, 25212,   355, 12489,  1821, 15984,   373,  7441,\n",
       "          2030,  9794,   658,   420,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt_tuning_model.generate(**ipt, max_length=256, do_sample=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c786e2-2d87-4555-bdd6-fb9f12dc6308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 考试有哪些好的技巧\n",
      "\n",
      "Assistant: 阅读理解作为外语考试的阅读部分，通常考察考生的语言知识、语言运用能力和语言综合运用能力等多个方面。阅读理解一般有以下几个方面：\n",
      "1、首先要懂得阅读的材料。在阅读材料时应该认真审读，做到“通读一遍”，以获得读懂材料的基本信息，“精读一段”，明确文段、段间或段内的主要内容和难点，然后再进行细读、慢读、浏览，以获得理解内容的关键信息。\n",
      "2、在阅读材料的过程中，对每一句话、每一半句、每一段，要重点地理解，不能单纯地根据句型、语法规则理解，必须重视句的语境、语体的整体性和内容的准确性，还要根据文章的结构、文章的中心思想、作者对文章作者的态度以及对文章内容的认识等方面的信息进行理解。\n",
      "3、阅读的材料应适合自己的阅读水平和阅读习惯，不能“机械”阅读。要按照自己所学的语种和教学大纲的要求及本学段的要求阅读所选的材料，不要看一些陌生的书或是读一些没有营养的文章，以免误判和理解错误。\n",
      "4、阅读的材料应具有\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt_tuning_model.generate(**ipt, max_length=256, do_sample=True)[0], \n",
    "                 skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359b117a-1ee7-4966-94e1-b1769dfba929",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`create_and_replace` does not support prompt learning and adaption prompt yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 通过外挂的方式进行加载\u001b[39;00m\n\u001b[1;32m      4\u001b[0m origin_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangboat/bloom-1b4-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/autodl-tmp/bloom-1b4-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m pp_model \u001b[38;5;241m=\u001b[39m \u001b[43morigin_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/autodl-tmp/prompt_tuning_bloom_1b4/save_pretrained\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m pp_model\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/integrations/peft.py:195\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    189\u001b[0m         peft_model_id,\n\u001b[1;32m    190\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[1;32m    192\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Create and add fresh new adapters into the model.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m \u001b[43minject_adapter_in_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_peft_config_loaded:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_peft_config_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/peft/mapping.py:169\u001b[0m, in \u001b[0;36minject_adapter_in_model\u001b[0;34m(peft_config, model, adapter_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mA simple API to create and inject adapter in-place into a model. Currently the API does not support prompt learning\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mmethods and adaption prompt. Make sure to have the correct `target_names` set in the `peft_config` object. The API\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m        The name of the adapter to be injected, if not provided, the default adapter name is used (\"default\").\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning \u001b[38;5;129;01mor\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_adaption_prompt:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`create_and_replace` does not support prompt learning and adaption prompt yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m PEFT_TYPE_TO_TUNER_MAPPING\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`inject_adapter_in_model` does not support \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mpeft_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m yet. Please use `get_peft_model`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `create_and_replace` does not support prompt learning and adaption prompt yet."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 通过外挂的方式进行加载\n",
    "\n",
    "\n",
    "origin_model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-1b4-zh\", cache_dir=\"/root/autodl-tmp/bloom-1b4-zh\")\n",
    "\n",
    "\n",
    "pp_model = origin_model.load_adapter(\"/root/autodl-tmp/prompt_tuning_bloom_1b4/save_pretrained\")\n",
    "\n",
    "\n",
    "pp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340d6ba-6cf8-4ab2-9c8a-981681de5f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
