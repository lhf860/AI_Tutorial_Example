{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2478842d-72b5-4884-abc2-3d942d04e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb9fd256-aae3-4c56-9c44-e0efc4899e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "介绍使用vLLM部署的模型\n",
    "\"\"\"\n",
    "\n",
    "llm = VLLMOpenAI(openai_api_key=\"abc\", openai_api_base=\"http://localhost:8000/v1\",\n",
    "                model_name=\"qwen2_5-7b-instruct\", temperature=0.1, max_tokens=512\n",
    "                 # model_kwargs={\"max_tokens\": 512}\n",
    "                )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3eab1b-f6b4-4ff9-b69b-d0cfe86de65b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"你能做什么? 请逐条介绍\"\n",
    "\n",
    "print(llm.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d2d571e-9cdb-4860-8911-0acaba2e528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatc4dfba47-8566-42fa-a6a6-35303c0ba167', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='当然，我可以帮助您完成多种任务。以下是我能够处理的一些常见请求：\\n\\n1. 回答各种问题：无论是科学、技术、文化、历史等领域的问题。\\n2. 提供信息和数据：包括但不限于天气预报、新闻摘要、统计数据等。\\n3. 语言翻译：将一种语言的内容翻译成另一种语言。\\n4. 写作辅助：提供文章构思、润色段落或生成创意内容。\\n5. 学习辅导：解释概念、解答学习中的疑惑。\\n6. 日常生活建议：如健康饮食建议、旅行规划等。\\n7. 技术支持：解决计算机使用中遇到的技术问题。\\n8. 情感支持：倾听并提供情感上的安慰与建议。\\n9. 游戏娱乐：参与简单的文字游戏或聊天互动。\\n\\n请注意，尽管我功能强大，但也有局限性，比如对于非常复杂的专业领域知识可能无法给出最准确的答案；在处理个人隐私相关的信息时需要特别小心保护用户隐私安全。如果您有任何具体需求或问题，欢迎随时提问！', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727269142, model='qwen2.5-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=220, prompt_tokens=35, total_tokens=255, completion_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "介绍使用xinference部署模型的使用，分为两种方式：\n",
    "1、openai的客户端\n",
    "2、langchain继承的\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import openai\n",
    "\n",
    "client = openai.Client(api_key=\"abc\", base_url=\"http://localhost:9997/v1\")\n",
    "\n",
    "response = client.chat.completions.create(model=\"qwen2.5-instruct\",\n",
    "                                          messages=[{\"role\": \"user\", \"content\": \"你会做什么，请分条列举\"}], \n",
    "                                          max_tokens=512,\n",
    "                                          temperature=0.1\n",
    "                                         )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9dac0d9-f0cd-435a-a73c-d5b985605a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chat7172f6ad-d9d8-416f-a5ba-d1c7e8fd1e7a', 'object': 'chat.completion', 'created': 1727269809, 'model': 'qwen2.5-instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"The largest animal currently living on Earth is the blue whale (Balaenoptera musculus). Here are some key facts about it:\\n\\n1. Size: Blue whales can grow up to 100 feet (30 meters) in length and weigh as much as 200 tons (or 400,000 pounds).\\n\\n2. Heart: The heart of a blue whale alone can be as large as a small car.\\n\\n3. Diet: Despite their massive size, blue whales feed almost exclusively on tiny shrimp-like creatures called krill.\\n\\n4. Species: There are three subspecies of blue whale, each found in different parts of the world's oceans.\\n\\n5. Extinction risk: Historically, blue whales were heavily hunted, but they're now protected under international law, though populations remain endangered due to past whaling activities and other threats like ship strikes and climate change.\\n\\nWhile there have been claims that certain extinct animals like dinosaurs or mammoths might have been larger, modern science has confirmed that the blue whale is indeed the largest known animal ever to exist.\"}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 25, 'completion_tokens': 221, 'total_tokens': 246}}\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/integrations/providers/xinference/\n",
    "\n",
    "\n",
    "\n",
    "from xinference.client import RESTfulClient\n",
    "xinference_restclient = RESTfulClient(base_url=\"http://localhost:9997\")\n",
    "xinference_model = xinference_restclient.get_model(\"qwen2.5-instruct\")\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"你会做什么，请分条列举\"}]\n",
    "res = xinference_model.chat(messages, generate_config={\"max_tokens\": 512,\"temperature\": 0.7})\n",
    "print(res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28ed3abe-5183-425b-9d64-f8890acfb21d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m            RESTfulChatModelHandle\n",
       "\u001b[0;31mString form:\u001b[0m     <xinference.client.restful.restful_client.RESTfulChatModelHandle object at 0x7f0a28f97910>\n",
       "\u001b[0;31mFile:\u001b[0m            ~/autodl-tmp/env/torch/lib/python3.10/site-packages/xinference/client/restful/restful_client.py\n",
       "\u001b[0;31mSource:\u001b[0m         \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mRESTfulChatModelHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESTfulGenerateModelHandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mgenerate_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LlamaCppGenerateConfig\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PytorchGenerateConfig\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ChatCompletion\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ChatCompletionChunk\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Given a list of messages comprising a conversation, the model will return a response via RESTful APIs.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Parameters\u001b[0m\n",
       "\u001b[0;34m        ----------\u001b[0m\n",
       "\u001b[0;34m        messages: List[Dict]\u001b[0m\n",
       "\u001b[0;34m            A list of messages comprising the conversation so far.\u001b[0m\n",
       "\u001b[0;34m        tools: Optional[List[Dict]]\u001b[0m\n",
       "\u001b[0;34m            A tool list.\u001b[0m\n",
       "\u001b[0;34m        generate_config: Optional[Union[\"LlamaCppGenerateConfig\", \"PytorchGenerateConfig\"]]\u001b[0m\n",
       "\u001b[0;34m            Additional configuration for the chat generation.\u001b[0m\n",
       "\u001b[0;34m            \"LlamaCppGenerateConfig\" -> configuration for llama-cpp-python model\u001b[0m\n",
       "\u001b[0;34m            \"PytorchGenerateConfig\" -> configuration for pytorch model\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Returns\u001b[0m\n",
       "\u001b[0;34m        -------\u001b[0m\n",
       "\u001b[0;34m        Union[\"ChatCompletion\", Iterator[\"ChatCompletionChunk\"]]\u001b[0m\n",
       "\u001b[0;34m            Stream is a parameter in generate_config.\u001b[0m\n",
       "\u001b[0;34m            When stream is set to True, the function will return Iterator[\"ChatCompletionChunk\"].\u001b[0m\n",
       "\u001b[0;34m            When stream is set to False, the function will return \"ChatCompletion\".\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Raises\u001b[0m\n",
       "\u001b[0;34m        ------\u001b[0m\n",
       "\u001b[0;34m        RuntimeError\u001b[0m\n",
       "\u001b[0;34m            Report the failure to generate the chat from the server. Detailed information provided in error message.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self._base_url}/v1/chat/completions\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrequest_body\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_uid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mrequest_body\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tools\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mgenerate_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mrequest_body\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_config\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgenerate_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth_headers\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"Failed to generate chat completion, detail: {_get_error_string(response)}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mstreaming_response_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "A sync model interface (for RESTful client) which provides type hints that makes it much easier to use xinference\n",
       "programmatically."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xinference_model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b445fc6-e8b1-4620-9c1b-67668e27ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from langchain_community.llms import Xinference\n",
    "\n",
    "xinference_llm = Xinference(server_url=\"http://localhost:9997\", model_uid=\"qwen2.5-instruct\")  # qwen2.5-instruct\n",
    "\n",
    "# rr = xinference_llm()\n",
    "\n",
    "xinference_llm??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec036ca-3d1f-4e34-9135-9719288f0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeed",
   "language": "python",
   "name": "deepspeed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
