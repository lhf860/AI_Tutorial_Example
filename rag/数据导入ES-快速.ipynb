{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c36395-b1cf-417e-a4d9-d08dcf13563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "import openai\n",
    "from langchain_elasticsearch import ElasticsearchEmbeddings\n",
    "\n",
    "from xinference.client import Client\n",
    "\n",
    "import openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796ca092-a789-4c0a-94c5-635a0c7587b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原标题：迈瑞医疗诉宝莱特侵权 共计索赔3000万元上证报中国证券网讯（记者 黎灵希）8月14日晚，宝莱特披露，公司于近日收到福建省福州市中级人民法院送达的传票及《民事起诉状》等法律文书，涉及3起专利纠纷案件。具体来看，3起案件的原告均为迈瑞医疗，被告一、被告二均分别是宝莱特和南平晨瑞医疗器械有限公司（以下简称“晨瑞公司”）。3起案件中，迈瑞医疗共计向宝莱特索赔3000万元。截至8月14日，上述案件已立案受理尚未开庭审理。在案件一中，迈瑞医疗认为，其与深圳迈瑞科技有限公司是专利号为ZL201780051006.3、名称为“医疗设备”的发明专利权人。宝莱特制造、销售、许诺销售P1型号的监护仪，晨瑞公司销售上述型号的监护仪，落入迈瑞医疗专利权的保护范围，宝莱特和晨瑞公司应当承担专利侵权的法律责任。迈瑞医疗请求法院判令上述两被告立即停止侵权行为，宝莱特立即停止制造、销售、许诺销售P1型号的监护仪，销毁库存的上述产品的成品及半成品，销毁该产品的广告和宣传印刷资料，删除互联网上对该产品的宣传和广告。晨瑞公司立即停止销售P1型号的监护仪。并且，迈瑞医疗向宝莱特索赔1000万元。案件二和案件三涉及的发明专利亦与监护仪有关。在两个案件中，迈瑞医疗均未要求晨瑞公司进行经济赔偿，而是分别向宝莱特索赔1000万元。宝莱特在公告中称，鉴于相关案件尚未开庭审理或尚未结案，案件结果尚存在不确定性，其对公司本期利润或期后利润的影响存在不确定性。公司预计本次诉讼案件不影响公司日常生产经营。据悉，宝莱特的产品主要涵盖健康监测和血液净化两大领域，其中健康监测主营产品为监护仪设备、心电图机、脉搏血氧仪、中央监护系统、可穿戴医疗产品等。按2022年营业收入来看，监护仪产品在宝莱特同期营业收入中占比约31.81%。据悉，此次并非迈瑞医疗首次状告宝莱特。今年4月，宝莱特披露，迈瑞医疗向其发起了2起专利侵权诉讼，共计索赔1000万元。宝莱特在8月14日晚公告中称，这两起案件目前法院已受理，尚未开庭审理。返回搜狐，查看更多责任编辑：\n",
      "\n",
      "原标题：“法在身边”|专骗“跑分”团伙 五人“黑吃黑”被判刑极目新闻记者 赵贝通讯员 谭代勋 李红旭诈骗犯罪团伙在进行诈骗时，自己却被贼惦记，诈骗资金被洗劫一空，上演了一出“黑吃黑”。近日，湖北省咸丰县人民法院审理了该起案件，专骗“跑分”5人团伙被判刑。案情显示，2022年7月初，杨某、刘某、王某、陈某与黄某（另案处理）、谢某（另案处理）在福建省龙岩市商议将银行卡提供给“跑分”团伙用于网络诈骗等违法犯罪活动的支付结算，趁“跑分”团伙大额资金转入之机用手机软件将资金转出占有。杨某、刘某负责组织实施相关事宜，刘某联系“跑分”团伙，谢某联系官某提供银行卡，黄某用手机软件绑定官某的银行卡。尔后，官某到刘某联系的“跑分”团伙“跑分”。2022年7月4日，“跑分”人员使用官某的银行卡进行支付结算，杨某、刘某、王某、陈某等人见官某的银行卡有大量资金转入，便由黄某等人使用手机软件从中转出11万元，次日，官某趁“跑分”团伙成员不备逃出。事后，杨某、刘某、王某、陈某、官某、谢某、黄某将该笔钱进行了分赃。法院审理认为，被告人杨某、刘某、王某、陈某、官某以非法占有为目的，秘密窃取他人财物共计人民币110000元，数额巨大，五被告人的行为均已构成盗窃罪；被告人杨某、刘某、王某、陈某、官某明知他人利用信息网络实施犯罪，为其提供支付结算等服务，情节严重，五被告人的行为均已构成帮助信息网络犯罪活动罪。综合考虑杨某、刘某、王某、陈某、官某是否具有坦白、犯罪前科、自愿认罪认罚、退缴违法所得及在共同犯罪中所起作用大小等量刑情节，分别判处杨某等五人有期徒刑一年六个月至三年八个月不等，并处罚金4000元至11000元不等。（来源：极目新闻）返回搜狐，查看更多责任编辑：\n",
      "\n",
      "原标题：广西外国语学院：已报警！8月14日，#当你被缅甸语专业录取#登上热搜。近日，短视频平台上一位叫“蒙绍奇”的学生，显示被广西外国语学院缅甸语（中外合作办学）录取，评论区不少人调侃“答应我毕了业之后不要给我打电话哦”，质疑其将从事电信诈骗。网友称自己被广西外国语学院缅甸语（中外合作办学）录取。图/社交媒体8月14日下午，记者联系到广西外国语学院东南亚语言文化学院，工作人员称“关于蒙绍奇这个名字，在我们今年本科录取数据当中并未查询到。包括他发布的录取通知书，我们的缅甸语招生中没有中外合作办学方向的。”工作人员表示没有这个学生，也没有这个方向，目前已向警方报警。对于网友们将缅甸语专业与从事电信诈骗关联起来的现象，工作人员回应，“不是说缅甸语毕业了就要去缅甸，虽然是缅甸语的专业，但是毕业之后的就业方向有很多。”根据广西外国语学院东南亚语言文化学院发布的缅甸语专业介绍，广西外国语学院缅甸语专业经教育部批准于2021年开始招收本科生，实行3+1培养模式，即3年在国内学习，1年在国外学习；毕业后可在外经贸、教育、旅游、外事、文化、新闻出版等行业部门从事翻译、教学、管理等工作；亦可考取国家公务员或国内外攻读硕士学位或自主创业。8月14日，@广西外国语学院发布声明：近日，网络上出现了将广西外国语学院缅甸语专业和缅甸电信诈骗恶意关联的帖子和短视频，主要内容为：“高考生被广西外国语学院录取，亲戚朋友看到专业，有点瑟瑟发抖”“我已经拿到缅甸语的录取通知书了”等。帖子出现后，学校高度重视，对有关帖子及相关内容进行核查。经查，相关帖子是将我校的录取通知书中的学生姓名和录取专业P图替换，所述内容均为捏造。此行为严重扰乱了学校招生秩序，对学校的声誉造成了不良影响。我校已经搜集了相关证据并向公安机关报案，并保留追究其法律责任的权利。来源：九派新闻、@广西外国语学院版权归原作者所有责任编辑：翟晓晨审核：张雷转载是一种动力 分享是一种美德返回搜狐，查看更多责任编辑：\n",
      "\n",
      "原标题：离婚逃债？法院：离婚协议相关约定无效！极目新闻记者 赵贝通讯员 周云实习生 汪旸家庭欠债，夫妻离婚，并将大部分财产分给孩子，导致债权人的权利难以实现。近日，湖北省宜昌市西陵区人民法院审结了一起案件，判决两位被告人离婚协议中的财产分割条款无效。案情显示，张某和李某于1986年登记结婚，于1988年生育一子。2020年8月，张某和李某在民政局协议离婚，《离婚协议书》中第五条约定：“关于财产分割：1.住房：位于某某地段的A住房归张某所有，位于某某地段的B住房、位于某某地段的C住房归儿子所有。2.小车归李某所有。”2018年，李某因经营需要，向王某某借款120万元，后因未还款，王某某将张某、李某诉至法院，要求两人还款。法院判决张某、李某须向王某某还款120万元及利息。2021年，王某某得知张某、李某离婚，查询财产线索时发现《离婚协议书》，认为张某、李某恶意逃避债务，遂向法院起诉要求确认《离婚协议书》中关于财产的分割条款无效。被告张某、李某辩称：《离婚协议书》是双方真实意思的表示，是考虑到孩子成长需要，及妻子张某对家庭的贡献，而作出的约定，王某某作为家庭之外的人，无权提出质疑。法条链接法院审理认为，虽然张某、李某的协议离婚属于人身性质，但就财产分割部分仍然应适用《民法典》合同编相关规定，债权人有选择撤销权或者合同无效确认权。本案中，张某、李某离婚时，将大部分财产分给了儿子，其夫妻两人仅有一套住房（且实际居住，不便执行）、一辆汽车，给债权人王某某追偿债务造成实质影响，导致债权落空。债权人王某某选择合同无效确认权，符合法律规定，应予支持。故判决：确认张某、李某《离婚协议书》中的第五条无效。（来源：极目新闻）返回搜狐，查看更多责任编辑：\n",
      "\n",
      "原标题：跑动2次→0次！杨浦这个业务实现全程线上办理近日，上海新光医院法人李先生向杨浦区卫健委监督所审核科工作人员咨询“医疗机构执业登记（备案）-增加诊疗科目”业务的办理流程时得知，该业务已实现“一网通办”平台线上办理，后续，李先生通过“一网通办”平台的指引，全程在线上完成了该业务的预审提交及最终审核，最后，通过快递的方式成功领取到了更新后的证件。“一网通办”平台办事流程的便捷高效让李先生连连赞叹：“居然这么方便，真的为企业着想！”几日后，李先生因十分满意这次的办事体验，为杨浦区卫健委监督所审核科赠予了一面锦旗。据悉，原先办理“医疗机构执业登记（备案）-增加诊疗科目”业务时，办事人需到线下窗口跑2次进行材料审核，第一次是现场审核的提前服务阶段，第二次是业务系统审批阶段。而如今，在现场审核的提前服务阶段，工作人员只需在收到一网通办平台上办事人上传的办事材料后就可直接打印审核，无需办事人到线下窗口亲自办理。接下来，在审核通过后的业务系统审批阶段，办事人也只需将材料原件通过物流的方式快递至窗口，由窗口制好证后与材料原件一并通过物流的方式快递至办事人手中，免去了办事人终审领证的跑动，整个流程中，将办事人跑动次数由原先的2次缩减为0次。后续，杨浦区也将进一步推进政务服务线上线下标准统一、服务同质；推动窗口端、电脑端、移动端、自助端四端联动、功能升级；提升服务效能，以企业群众满意度为第一标准，实现政务服务触手可达、就近可办。来源：上海杨浦返回搜狐，查看更多责任编辑：\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/root/autodl-tmp/rag_practice/CRUD_RAG/data/80000_docs\"\n",
    "\n",
    "doc_100 = 100\n",
    "all_docs = []\n",
    "for file_name in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    with open(file_path) as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            \n",
    "            linelist = line.split(\"正文：\")\n",
    "            if len(linelist) > 1:\n",
    "                content = linelist[1]\n",
    "            else:\n",
    "                content = line\n",
    "            # content = line.split(\"正文：\")[1]\n",
    "            # print(content)\n",
    "            all_docs.append(content)\n",
    "            # if len(all_docs) > doc_100:\n",
    "            #     break\n",
    "for doc in all_docs[:5]:\n",
    "    print(doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633d3e15-fcdc-455a-a722-e132aaacd774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'autodl-container-5525458f7b-77aecf75', 'cluster_name': 'elasticsearch', 'cluster_uuid': '1pL5dQRgRheVp50FcnVuIg', 'version': {'number': '8.15.1', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': '253e8544a65ad44581194068936f2a5d57c2c051', 'build_date': '2024-09-02T22:04:47.310170297Z', 'build_snapshot': False, 'lucene_version': '9.11.1', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ELASTIC_PASSWORD = \"70wnhd8X35o4TnJAYqYk\"\n",
    "\n",
    "# Create the client instance\n",
    "client = Elasticsearch(\"http://localhost:9200\",\n",
    "    # basic_auth=(\"elastic\", ELASTIC_PASSWORD)\n",
    ")\n",
    "print(client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a5b52-fa62-45bb-8701-719c3102c879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "model_uid = \"qwen2.5-instruct\"\n",
    "\n",
    "llm_client =openai.Client(api_key=\"ab\", base_url=\"http://localhost:9997/v1\")    # 客户端要带上v1\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=model_uid,\n",
    "    messages=[\n",
    "        {\n",
    "            \"content\": \"大模型的核心技术有哪些\",\n",
    "            \"role\": \"user\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(response)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431a6cde-164d-4a88-a1c1-0f1b5ad214b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19262/4006526791.py:12: LangChainPendingDeprecationWarning: The class `ElasticsearchStore` will be deprecated in a future version. Use :class:`~Use class in langchain-elasticsearch package` instead.\n",
      "  es_store = ElasticsearchStore(index_name=\"rag-index\", es_url=\"http://localhost:9200\", embedding=xinference_embeddings)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# https://python.langchain.com/docs/integrations/text_embedding/xinference/\n",
    "from langchain.embeddings import XinferenceEmbeddings\n",
    "from langchain_community.vectorstores import ElasticsearchStore\n",
    "\n",
    "\n",
    "# from langchain.embeddings import HuggingFaceBgeEmbeddings, HuggingFaceEmbeddings, HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "# 路由中不要包含v1\n",
    "xinference_embeddings = XinferenceEmbeddings(server_url=\"http://localhost:9997\",model_uid=\"bge-base-zh-v1.5\")\n",
    "\n",
    "\n",
    "es_store = ElasticsearchStore(index_name=\"rag-index\", es_url=\"http://localhost:9200\", embedding=xinference_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "043fbabd-be50-496c-9fe8-98204d71d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_dir = \"/root/autodl-tmp/rag_practice/CRUD_RAG/data/80000_docs\"\n",
    "\n",
    "# doc_100 = 100\n",
    "# all_docs = []\n",
    "# for file_name in os.listdir(data_dir):\n",
    "#     file_path = os.path.join(data_dir, file_name)\n",
    "#     with open(file_path) as fin:\n",
    "#         for line in fin:\n",
    "#             line = line.strip()\n",
    "            \n",
    "#             linelist = line.split(\"正文：\")\n",
    "#             if len(linelist) > 1:\n",
    "#                 content = linelist[1]\n",
    "#             else:\n",
    "#                 content = line\n",
    "#             # content = line.split(\"正文：\")[1]\n",
    "#             # print(content)\n",
    "#             all_docs.append(content)\n",
    "#             # if len(all_docs) > doc_100:\n",
    "#             #     break\n",
    "# for doc in all_docs[:5]:\n",
    "#     print(doc)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51af69a3-e143-42f4-98d3-5bee89d27ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TextSplitter, RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter, TextSplitter, TokenTextSplitter\n",
    "# from langchain_community.text_splitter import SentenceSplitter\n",
    "\n",
    "\n",
    "\n",
    "# text_spliter = SentenceTransformersTokenTextSplitter(chunk_size=128, chunk_overlap=50)\n",
    "\n",
    "recursive_text_split = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02f5d7b-59b2-408b-bf1d-6d9c71760242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91964/91964 [01:04<00:00, 1428.87it/s] \n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_one_doc(doc: str):\n",
    "    doc_id = str(uuid.uuid3(uuid.NAMESPACE_DNS, doc))\n",
    "\n",
    "    # doc_info  = {\"doc_id\": doc_id, \"\"}\n",
    "    \n",
    "    chunks = recursive_text_split.split_text(doc)\n",
    "    chunk_metadata_list = []\n",
    "    for chunk_index, chunk in enumerate(chunks):\n",
    "        chunk_metadata = {\"ids\": doc_id, \"chunk_index\": chunk_index, \"chunk_text\": chunk}\n",
    "        chunk_metadata_list.append(chunk_metadata)\n",
    "\n",
    "    return chunks, chunk_metadata_list, [doc_id] * len(chunks)\n",
    "    \n",
    "\n",
    "def preprocess_docs():\n",
    "\n",
    "    all_chunk_text, all_chunk_text_meta, all_doc_ids = [],  [],  []\n",
    "\n",
    "    for doc in tqdm(all_docs):\n",
    "        chunks, chunk_metadata_list, doc_ids = process_one_doc(doc)\n",
    "        all_chunk_text.extend(chunks)\n",
    "        all_chunk_text_meta.extend(chunk_metadata_list)\n",
    "        all_doc_ids.extend(doc_ids)\n",
    "    return all_chunk_text, all_chunk_text_meta, all_doc_ids\n",
    "\n",
    "\n",
    "# process_one_doc(all_docs[1])\n",
    "# print(len(all_docs))  # 91964\n",
    "\n",
    "all_chunk_text, all_chunk_text_meta, all_doc_ids = preprocess_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf02e6aa-dff6-4b1f-8820-279e16b317d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 23:02:18,671 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "Inference Embeddings: 100%|██████████| 767/767 [04:29<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_chunk_text))\n",
    "\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download(\"AI-ModelScope/bge-base-zh-v1.5\", revision='master', cache_dir=\"./bge-base-zh-v1.5\")\n",
    "\n",
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "embedding_model = FlagModel(model_dir, query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\",\n",
    "                  use_fp16=True)\n",
    "embeddings = embedding_model.encode_corpus(all_chunk_text, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3380c31c-a494-4a17-9f29-a63e6398c5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392578\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# res = es_store.add_texts(all_chunk_text, all_chunk_text_meta, all_doc_ids)\n",
    "# print(len(res))\n",
    "\n",
    "text_embeddings = [(all_chunk_text[i], embeddings[i]) for i in range(len(embeddings))]\n",
    "\n",
    "res = es_store.add_embeddings(text_embeddings=text_embeddings, metadatas=all_chunk_text_meta, ids=all_doc_ids)\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5de70-f406-45a7-b1ac-33defbdb6ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(xinference_embeddings)\n",
    "\n",
    "# xinference_embeddings.aembed_documents??\n",
    "\n",
    "# chunk_embedding_list = []\n",
    "\n",
    "# chunk_length = len(all_chunk_text)\n",
    "# batch_size = 128\n",
    "# for start_index in tqdm(range(0, len(all_chunk_text), batch_size)):\n",
    "#     end_index = min(start_index + batch_size, chunk_length)\n",
    "#     batch_embeddings = xinference_embeddings.embed_documents(all_chunk_text[start_index:end_index])\n",
    "    \n",
    "#     chunk_embedding_list.extend(batch_embeddings)\n",
    "    \n",
    "\n",
    "# print(len(chunk_embedding_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0f80a-2842-4750-bf55-43b501a5ad5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download(\"AI-ModelScope/bge-base-zh-v1.5\", revision='master', cache_dir=\"./bge-base-zh-v1.5\")\n",
    "\n",
    "# from FlagEmbedding import FlagModel\n",
    "\n",
    "# embedding_model = FlagModel(model_dir, query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\",\n",
    "#                   use_fp16=True)\n",
    "# embeddings = embedding_model.encode_corpus(all_chunk_text, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6de206-a532-4eb3-82a2-9fe3908085b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
